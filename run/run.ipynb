{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-859628a87e5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdashboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/production/API/lib/dashboard.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m  \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/production/API/lib/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from lib.dashboard import *\n",
    "from lib.graph import *\n",
    "from lib.models import *\n",
    "from lib.preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data only contain rows with: Store == 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labbrand/Desktop/API/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3183: DtypeWarning:\n",
      "\n",
      "Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data only contain rows with: Store == 1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_address= '../projects/data/train.csv'\n",
    "y_variable= 'Sales'\n",
    "p=preprocess()\n",
    "p.set_parameter('address', file_address)\n",
    "p.set_parameter('y_variable', y_variable)\n",
    "p.read_file()\n",
    "p.filterin('Store',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.data.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan replaced with 0. Can pass something other than 0\n",
      "Variables successfully selected\n",
      "File does not need time splitting\n",
      "Actual Unit as y variable successfully chosen\n",
      "Categoricol to nominal done!\n",
      "Dim calculated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dimensions calculated'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_address= '../projects/data/garan.csv'\n",
    "y_variable= 'Actual Unit'\n",
    "p1=preprocess()\n",
    "p1.set_parameter('address', file_address)\n",
    "p1.set_parameter('y_variable', y_variable)\n",
    "p1.read_file()\n",
    "p1.replace_nan(0)\n",
    "p1.variables()\n",
    "p1.split_time()\n",
    "p1.choose_y()\n",
    "p1.category_to_nominal()\n",
    "p1.calculate_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../lib/models.py:108: UserWarning:\n",
      "\n",
      "The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "\n",
      "../lib/models.py:112: UserWarning:\n",
      "\n",
      "Update your `Dense` call to the Keras 2 API: `Dense(1000, kernel_initializer=\"uniform\")`\n",
      "\n",
      "../lib/models.py:115: UserWarning:\n",
      "\n",
      "Update your `Dense` call to the Keras 2 API: `Dense(200, kernel_initializer=\"uniform\")`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m=NN_with_EE()\n",
    "project_name= 'garam'\n",
    "m.build_model(project_name, p1.p['io_dim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is training\n",
      "Train on 30025 samples, validate on 900 samples\n",
      "Epoch 1/10\n",
      "19820/30025 [==================>...........] - ETA: 8s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a14943e0b0bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/API/lib/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    168\u001b[0m                        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nb_ephoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'checkpointer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_val_for_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                        )\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1217\u001b[0m                         \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m.fit(np.array(p1.data[:-900]),np.array(p1.data_y[:-900]),np.array(p1.data[-900:]), np.array(p1.data_y[-900:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file\n",
      "Nan replaced with 0. Can pass something other than 0\n",
      "Variables successfully selected\n",
      "File does not need time splitting\n",
      "Actual Unit as y variable successfully chosen\n",
      "Categoricol to nominal done!\n",
      "Dim calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../lib/models.py:108: UserWarning:\n",
      "\n",
      "The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "\n",
      "../lib/models.py:112: UserWarning:\n",
      "\n",
      "Update your `Dense` call to the Keras 2 API: `Dense(1000, kernel_initializer=\"uniform\")`\n",
      "\n",
      "../lib/models.py:115: UserWarning:\n",
      "\n",
      "Update your `Dense` call to the Keras 2 API: `Dense(200, kernel_initializer=\"uniform\")`\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/labbrand/Desktop/API/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Model is training\n",
      "Train on 27832 samples, validate on 3093 samples\n",
      "Epoch 1/10\n",
      "27780/27832 [============================>.] - ETA: 0s - loss: nanEpoch 00001: val_loss did not improve\n",
      "27832/27832 [==============================] - 21s 769us/step - loss: nan - val_loss: nan\n",
      "Epoch 2/10\n",
      "27810/27832 [============================>.] - ETA: 0s - loss: nanEpoch 00002: val_loss did not improve\n",
      "27832/27832 [==============================] - 21s 743us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/10\n",
      "15160/27832 [===============>..............] - ETA: 9s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7783b2c852b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y_variable'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_variable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/API/lib/dashboard.py\u001b[0m in \u001b[0;36mfind_best_model\u001b[0;34m(self, train_test_split, epoch, batch_size)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 rmse= self.model[model_name].fit(np.array(self.preprocess_object.data[:split]),\n\u001b[1;32m     76\u001b[0m                                  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                                  np.array(self.preprocess_object.data[split:]), np.array(self.preprocess_object.data_y[split:]))\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0;31m#update best model vraible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mmodel_rmse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/API/lib/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    168\u001b[0m                        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nb_ephoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'checkpointer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_val_for_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                        )\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[0;32m-> 1082\u001b[0;31m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[0m\u001b[1;32m   1083\u001b[0m             raise ValueError('Cannot feed value of shape %r for Tensor %r, '\n\u001b[1;32m   1084\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mis_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \"\"\"\n\u001b[1;32m    826\u001b[0m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mdims\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    557\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m\"\"\"Returns a list of Dimensions, or None if the shape is unspecified.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "project_name= 'Garan'\n",
    "project_type= 'p'\n",
    "file_address= '../projects/data/garan.csv'\n",
    "y_variable= 'Actual Unit'\n",
    "d=Dashboard(project_name, project_type)\n",
    "d.set_parameter('file_address', file_address)\n",
    "d.set_parameter('y_variable', y_variable)\n",
    "d.transform_data(filter_in=['Member Description', ])\n",
    "d.find_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/labbrand/Desktop/API/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3265: DtypeWarning:\n",
      "\n",
      "Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data only contain rows with: Store == 1\n",
      "Nan replaced with 0. Can pass something other than 0\n",
      "Variables successfully selected\n",
      "Removed and splitted Date!\n",
      "Sales as y variable successfully chosen\n",
      "Categoricol to nominal done!\n",
      "Dim calculated\n",
      "WARNING:tensorflow:From /Users/labbrand/Desktop/API/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/labbrand/Desktop/API/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Model is training\n",
      "Train on 826 samples, validate on 74 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 0.05708, saving model to ../projects/fairyMo/models/LSTM_weights.h5\n",
      " - 4s - loss: 0.1628 - val_loss: 0.0571\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 0.05708 to 0.03474, saving model to ../projects/fairyMo/models/LSTM_weights.h5\n",
      " - 3s - loss: 0.0426 - val_loss: 0.0347\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.03474 to 0.03242, saving model to ../projects/fairyMo/models/LSTM_weights.h5\n",
      " - 3s - loss: 0.0332 - val_loss: 0.0324\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.03242 to 0.03081, saving model to ../projects/fairyMo/models/LSTM_weights.h5\n",
      " - 3s - loss: 0.0316 - val_loss: 0.0308\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.03081 to 0.03009, saving model to ../projects/fairyMo/models/LSTM_weights.h5\n",
      " - 2s - loss: 0.0300 - val_loss: 0.0301\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.03009 to 0.02880, saving model to ../projects/fairyMo/models/LSTM_weights.h5\n",
      " - 2s - loss: 0.0290 - val_loss: 0.0288\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 2s - loss: 0.0284 - val_loss: 0.0293\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 2s - loss: 0.0281 - val_loss: 0.0306\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss improved from 0.02880 to 0.02642, saving model to ../projects/fairyMo/models/LSTM_weights.h5\n",
      " - 2s - loss: 0.0266 - val_loss: 0.0264\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 2s - loss: 0.0260 - val_loss: 0.0284\n",
      "Result on validation data:  None\n",
      "Training Done\n"
     ]
    }
   ],
   "source": [
    "project_name= 'fairyMo'\n",
    "project_type= 'f'\n",
    "file_address= '../projects/data/train.csv'\n",
    "y_variable= 'Sales'\n",
    "d1= Dashboard(project_name, project_type)\n",
    "d1.set_parameter('file_address', file_address)\n",
    "d1.set_parameter('y_variable', y_variable)\n",
    "d1.transform_data(filter_in=['Store',1])\n",
    "d1.find_best_model(.9,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.39598138e+03, 2.56207610e+03, 1.81940324e+03, 1.78172000e+03,\n",
       "       2.12917437e+03, 2.80872433e+03, 5.59748268e-01, 2.11686450e+03,\n",
       "       2.32536322e+03, 1.69115091e+03, 1.45591278e+03, 1.94722876e+03,\n",
       "       2.76073584e+03, 1.08406912e+00, 2.12178828e+03, 2.32275552e+03,\n",
       "       1.53007297e+03, 1.41755264e+03, 1.88002579e+03, 2.75098464e+03,\n",
       "       2.80668635e+00, 2.12516003e+03, 2.37042838e+03, 1.53645322e+03,\n",
       "       1.34588167e+03, 1.93902349e+03, 2.69653651e+03, 5.01518946e+00,\n",
       "       2.15250132e+03, 2.32377119e+03])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.forecast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name= 'rossman2'\n",
    "m= XGBoost()\n",
    "m.build_model(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:7.09212\n",
      "[1]\ttrain-rmse:6.95055\n",
      "[2]\ttrain-rmse:6.81198\n",
      "[3]\ttrain-rmse:6.67613\n",
      "[4]\ttrain-rmse:6.54289\n",
      "[5]\ttrain-rmse:6.4125\n",
      "[6]\ttrain-rmse:6.28467\n",
      "[7]\ttrain-rmse:6.15935\n",
      "[8]\ttrain-rmse:6.0366\n",
      "[9]\ttrain-rmse:5.91611\n",
      "[10]\ttrain-rmse:5.79805\n",
      "[11]\ttrain-rmse:5.68236\n",
      "[12]\ttrain-rmse:5.56904\n",
      "[13]\ttrain-rmse:5.4582\n",
      "[14]\ttrain-rmse:5.34953\n",
      "[15]\ttrain-rmse:5.2429\n",
      "[16]\ttrain-rmse:5.13839\n",
      "[17]\ttrain-rmse:5.03606\n",
      "[18]\ttrain-rmse:4.93573\n",
      "[19]\ttrain-rmse:4.83733\n",
      "[20]\ttrain-rmse:4.74094\n",
      "[21]\ttrain-rmse:4.64632\n",
      "[22]\ttrain-rmse:4.55367\n",
      "[23]\ttrain-rmse:4.463\n",
      "[24]\ttrain-rmse:4.37416\n",
      "[25]\ttrain-rmse:4.28712\n",
      "[26]\ttrain-rmse:4.20169\n",
      "[27]\ttrain-rmse:4.11796\n",
      "[28]\ttrain-rmse:4.03603\n",
      "[29]\ttrain-rmse:3.95558\n",
      "[30]\ttrain-rmse:3.87679\n",
      "[31]\ttrain-rmse:3.79959\n",
      "[32]\ttrain-rmse:3.72393\n",
      "[33]\ttrain-rmse:3.64977\n",
      "[34]\ttrain-rmse:3.57723\n",
      "[35]\ttrain-rmse:3.50605\n",
      "[36]\ttrain-rmse:3.43627\n",
      "[37]\ttrain-rmse:3.36794\n",
      "[38]\ttrain-rmse:3.301\n",
      "[39]\ttrain-rmse:3.23537\n",
      "[40]\ttrain-rmse:3.17104\n",
      "[41]\ttrain-rmse:3.10791\n",
      "[42]\ttrain-rmse:3.04605\n",
      "[43]\ttrain-rmse:2.9855\n",
      "[44]\ttrain-rmse:2.92616\n",
      "[45]\ttrain-rmse:2.87575\n",
      "[46]\ttrain-rmse:2.81856\n",
      "[47]\ttrain-rmse:2.76241\n",
      "[48]\ttrain-rmse:2.70747\n",
      "[49]\ttrain-rmse:2.65368\n",
      "[50]\ttrain-rmse:2.60094\n",
      "[51]\ttrain-rmse:2.54921\n",
      "[52]\ttrain-rmse:2.49851\n",
      "[53]\ttrain-rmse:2.4489\n",
      "[54]\ttrain-rmse:2.40021\n",
      "[55]\ttrain-rmse:2.35252\n",
      "[56]\ttrain-rmse:2.30593\n",
      "[57]\ttrain-rmse:2.26034\n",
      "[58]\ttrain-rmse:2.21536\n",
      "[59]\ttrain-rmse:2.17135\n",
      "[60]\ttrain-rmse:2.1282\n",
      "[61]\ttrain-rmse:2.08597\n",
      "[62]\ttrain-rmse:2.04446\n",
      "[63]\ttrain-rmse:2.00387\n",
      "[64]\ttrain-rmse:1.96401\n",
      "[65]\ttrain-rmse:1.92505\n",
      "[66]\ttrain-rmse:1.88682\n",
      "[67]\ttrain-rmse:1.84942\n",
      "[68]\ttrain-rmse:1.8127\n",
      "[69]\ttrain-rmse:1.77671\n",
      "[70]\ttrain-rmse:1.7416\n",
      "[71]\ttrain-rmse:1.70701\n",
      "[72]\ttrain-rmse:1.67321\n",
      "[73]\ttrain-rmse:1.64002\n",
      "[74]\ttrain-rmse:1.60751\n",
      "[75]\ttrain-rmse:1.57558\n",
      "[76]\ttrain-rmse:1.54433\n",
      "[77]\ttrain-rmse:1.51811\n",
      "[78]\ttrain-rmse:1.48806\n",
      "[79]\ttrain-rmse:1.45864\n",
      "[80]\ttrain-rmse:1.42971\n",
      "[81]\ttrain-rmse:1.40145\n",
      "[82]\ttrain-rmse:1.37375\n",
      "[83]\ttrain-rmse:1.34658\n",
      "[84]\ttrain-rmse:1.31987\n",
      "[85]\ttrain-rmse:1.29369\n",
      "[86]\ttrain-rmse:1.26819\n",
      "[87]\ttrain-rmse:1.24418\n",
      "[88]\ttrain-rmse:1.21955\n",
      "[89]\ttrain-rmse:1.19534\n",
      "[90]\ttrain-rmse:1.17183\n",
      "[91]\ttrain-rmse:1.14875\n",
      "[92]\ttrain-rmse:1.12601\n",
      "[93]\ttrain-rmse:1.10377\n",
      "[94]\ttrain-rmse:1.08197\n",
      "[95]\ttrain-rmse:1.06063\n",
      "[96]\ttrain-rmse:1.0397\n",
      "[97]\ttrain-rmse:1.01913\n",
      "[98]\ttrain-rmse:0.999115\n",
      "[99]\ttrain-rmse:0.979466\n",
      "[100]\ttrain-rmse:0.960177\n",
      "[101]\ttrain-rmse:0.941291\n",
      "[102]\ttrain-rmse:0.922683\n",
      "[103]\ttrain-rmse:0.904511\n",
      "[104]\ttrain-rmse:0.88663\n",
      "[105]\ttrain-rmse:0.869221\n",
      "[106]\ttrain-rmse:0.85208\n",
      "[107]\ttrain-rmse:0.835285\n",
      "[108]\ttrain-rmse:0.818925\n",
      "[109]\ttrain-rmse:0.802882\n",
      "[110]\ttrain-rmse:0.787156\n",
      "[111]\ttrain-rmse:0.771728\n",
      "[112]\ttrain-rmse:0.756628\n",
      "[113]\ttrain-rmse:0.741768\n",
      "[114]\ttrain-rmse:0.727183\n",
      "[115]\ttrain-rmse:0.712872\n",
      "[116]\ttrain-rmse:0.698889\n",
      "[117]\ttrain-rmse:0.685175\n",
      "[118]\ttrain-rmse:0.671701\n",
      "[119]\ttrain-rmse:0.65862\n",
      "[120]\ttrain-rmse:0.645654\n",
      "[121]\ttrain-rmse:0.632996\n",
      "[122]\ttrain-rmse:0.620599\n",
      "[123]\ttrain-rmse:0.608881\n",
      "[124]\ttrain-rmse:0.596914\n",
      "[125]\ttrain-rmse:0.585193\n",
      "[126]\ttrain-rmse:0.573758\n",
      "[127]\ttrain-rmse:0.562553\n",
      "[128]\ttrain-rmse:0.55155\n",
      "[129]\ttrain-rmse:0.540713\n",
      "[130]\ttrain-rmse:0.530262\n",
      "[131]\ttrain-rmse:0.519894\n",
      "[132]\ttrain-rmse:0.509906\n",
      "[133]\ttrain-rmse:0.499946\n",
      "[134]\ttrain-rmse:0.490208\n",
      "[135]\ttrain-rmse:0.480625\n",
      "[136]\ttrain-rmse:0.471248\n",
      "[137]\ttrain-rmse:0.462136\n",
      "[138]\ttrain-rmse:0.453138\n",
      "[139]\ttrain-rmse:0.444274\n",
      "[140]\ttrain-rmse:0.435682\n",
      "[141]\ttrain-rmse:0.42723\n",
      "[142]\ttrain-rmse:0.419093\n",
      "[143]\ttrain-rmse:0.410928\n",
      "[144]\ttrain-rmse:0.402917\n",
      "[145]\ttrain-rmse:0.395078\n",
      "[146]\ttrain-rmse:0.387503\n",
      "[147]\ttrain-rmse:0.380086\n",
      "[148]\ttrain-rmse:0.37282\n",
      "[149]\ttrain-rmse:0.365636\n",
      "[150]\ttrain-rmse:0.35867\n",
      "[151]\ttrain-rmse:0.351782\n",
      "[152]\ttrain-rmse:0.344969\n",
      "[153]\ttrain-rmse:0.338358\n",
      "[154]\ttrain-rmse:0.331853\n",
      "[155]\ttrain-rmse:0.325456\n",
      "[156]\ttrain-rmse:0.319198\n",
      "[157]\ttrain-rmse:0.313132\n",
      "[158]\ttrain-rmse:0.307165\n",
      "[159]\ttrain-rmse:0.301261\n",
      "[160]\ttrain-rmse:0.295482\n",
      "[161]\ttrain-rmse:0.289876\n",
      "[162]\ttrain-rmse:0.284337\n",
      "[163]\ttrain-rmse:0.279042\n",
      "[164]\ttrain-rmse:0.273781\n",
      "[165]\ttrain-rmse:0.268571\n",
      "[166]\ttrain-rmse:0.263518\n",
      "[167]\ttrain-rmse:0.258551\n",
      "[168]\ttrain-rmse:0.253749\n",
      "[169]\ttrain-rmse:0.24898\n",
      "[170]\ttrain-rmse:0.244404\n",
      "[171]\ttrain-rmse:0.239812\n",
      "[172]\ttrain-rmse:0.235314\n",
      "[173]\ttrain-rmse:0.230973\n",
      "[174]\ttrain-rmse:0.226594\n",
      "[175]\ttrain-rmse:0.222368\n",
      "[176]\ttrain-rmse:0.218234\n",
      "[177]\ttrain-rmse:0.214149\n",
      "[178]\ttrain-rmse:0.210181\n",
      "[179]\ttrain-rmse:0.20636\n",
      "[180]\ttrain-rmse:0.202628\n",
      "[181]\ttrain-rmse:0.198908\n",
      "[182]\ttrain-rmse:0.195266\n",
      "[183]\ttrain-rmse:0.19166\n",
      "[184]\ttrain-rmse:0.188166\n",
      "[185]\ttrain-rmse:0.18472\n",
      "[186]\ttrain-rmse:0.181355\n",
      "[187]\ttrain-rmse:0.178134\n",
      "[188]\ttrain-rmse:0.174921\n",
      "[189]\ttrain-rmse:0.171787\n",
      "[190]\ttrain-rmse:0.1687\n",
      "[191]\ttrain-rmse:0.165679\n",
      "[192]\ttrain-rmse:0.162705\n",
      "[193]\ttrain-rmse:0.15976\n",
      "[194]\ttrain-rmse:0.156913\n",
      "[195]\ttrain-rmse:0.154163\n",
      "[196]\ttrain-rmse:0.151545\n",
      "[197]\ttrain-rmse:0.148874\n",
      "[198]\ttrain-rmse:0.146262\n",
      "[199]\ttrain-rmse:0.14374\n",
      "[200]\ttrain-rmse:0.141217\n",
      "[201]\ttrain-rmse:0.138728\n",
      "[202]\ttrain-rmse:0.136321\n",
      "[203]\ttrain-rmse:0.133986\n",
      "[204]\ttrain-rmse:0.131679\n",
      "[205]\ttrain-rmse:0.129503\n",
      "[206]\ttrain-rmse:0.127351\n",
      "[207]\ttrain-rmse:0.125152\n",
      "[208]\ttrain-rmse:0.123049\n",
      "[209]\ttrain-rmse:0.120932\n",
      "[210]\ttrain-rmse:0.118912\n",
      "[211]\ttrain-rmse:0.116952\n",
      "[212]\ttrain-rmse:0.114993\n",
      "[213]\ttrain-rmse:0.113115\n",
      "[214]\ttrain-rmse:0.111265\n",
      "[215]\ttrain-rmse:0.109469\n",
      "[216]\ttrain-rmse:0.107712\n",
      "[217]\ttrain-rmse:0.105995\n",
      "[218]\ttrain-rmse:0.104343\n",
      "[219]\ttrain-rmse:0.10267\n",
      "[220]\ttrain-rmse:0.101063\n",
      "[221]\ttrain-rmse:0.099463\n",
      "[222]\ttrain-rmse:0.097966\n",
      "[223]\ttrain-rmse:0.096463\n",
      "[224]\ttrain-rmse:0.094971\n",
      "[225]\ttrain-rmse:0.093573\n",
      "[226]\ttrain-rmse:0.092162\n",
      "[227]\ttrain-rmse:0.090781\n",
      "[228]\ttrain-rmse:0.089401\n",
      "[229]\ttrain-rmse:0.088034\n",
      "[230]\ttrain-rmse:0.086701\n",
      "[231]\ttrain-rmse:0.085416\n",
      "[232]\ttrain-rmse:0.084165\n",
      "[233]\ttrain-rmse:0.082936\n",
      "[234]\ttrain-rmse:0.081748\n",
      "[235]\ttrain-rmse:0.080557\n",
      "[236]\ttrain-rmse:0.079377\n",
      "[237]\ttrain-rmse:0.078282\n",
      "[238]\ttrain-rmse:0.077167\n",
      "[239]\ttrain-rmse:0.07611\n",
      "[240]\ttrain-rmse:0.075014\n",
      "[241]\ttrain-rmse:0.073981\n",
      "[242]\ttrain-rmse:0.073004\n",
      "[243]\ttrain-rmse:0.072005\n",
      "[244]\ttrain-rmse:0.071034\n",
      "[245]\ttrain-rmse:0.070101\n",
      "[246]\ttrain-rmse:0.069191\n",
      "[247]\ttrain-rmse:0.06832\n",
      "[248]\ttrain-rmse:0.067425\n",
      "[249]\ttrain-rmse:0.066559\n",
      "[250]\ttrain-rmse:0.065695\n",
      "[251]\ttrain-rmse:0.064871\n",
      "[252]\ttrain-rmse:0.064092\n",
      "[253]\ttrain-rmse:0.063295\n",
      "[254]\ttrain-rmse:0.062509\n",
      "[255]\ttrain-rmse:0.061784\n",
      "[256]\ttrain-rmse:0.061055\n",
      "[257]\ttrain-rmse:0.060337\n",
      "[258]\ttrain-rmse:0.059631\n",
      "[259]\ttrain-rmse:0.05898\n",
      "[260]\ttrain-rmse:0.058316\n",
      "[261]\ttrain-rmse:0.057719\n",
      "[262]\ttrain-rmse:0.057111\n",
      "[263]\ttrain-rmse:0.056533\n",
      "[264]\ttrain-rmse:0.055913\n",
      "[265]\ttrain-rmse:0.05533\n",
      "[266]\ttrain-rmse:0.054772\n",
      "[267]\ttrain-rmse:0.054247\n",
      "[268]\ttrain-rmse:0.053733\n",
      "[269]\ttrain-rmse:0.053188\n",
      "[270]\ttrain-rmse:0.052666\n",
      "[271]\ttrain-rmse:0.052168\n",
      "[272]\ttrain-rmse:0.051652\n",
      "[273]\ttrain-rmse:0.051162\n",
      "[274]\ttrain-rmse:0.050698\n",
      "[275]\ttrain-rmse:0.050243\n",
      "[276]\ttrain-rmse:0.049843\n",
      "[277]\ttrain-rmse:0.04939\n",
      "[278]\ttrain-rmse:0.048972\n",
      "[279]\ttrain-rmse:0.048537\n",
      "[280]\ttrain-rmse:0.048083\n",
      "[281]\ttrain-rmse:0.047719\n",
      "[282]\ttrain-rmse:0.047327\n",
      "[283]\ttrain-rmse:0.046942\n",
      "[284]\ttrain-rmse:0.046559\n",
      "[285]\ttrain-rmse:0.046192\n",
      "[286]\ttrain-rmse:0.045854\n",
      "[287]\ttrain-rmse:0.045535\n",
      "[288]\ttrain-rmse:0.045207\n",
      "[289]\ttrain-rmse:0.044867\n",
      "[290]\ttrain-rmse:0.044567\n",
      "[291]\ttrain-rmse:0.044236\n",
      "[292]\ttrain-rmse:0.043894\n",
      "[293]\ttrain-rmse:0.043564\n",
      "[294]\ttrain-rmse:0.043265\n",
      "[295]\ttrain-rmse:0.043014\n",
      "[296]\ttrain-rmse:0.042706\n",
      "[297]\ttrain-rmse:0.042429\n",
      "[298]\ttrain-rmse:0.042178\n",
      "[299]\ttrain-rmse:0.041898\n",
      "[300]\ttrain-rmse:0.041622\n",
      "[301]\ttrain-rmse:0.041362\n",
      "[302]\ttrain-rmse:0.041092\n",
      "[303]\ttrain-rmse:0.040848\n",
      "[304]\ttrain-rmse:0.040621\n",
      "[305]\ttrain-rmse:0.04037\n",
      "[306]\ttrain-rmse:0.040158\n",
      "[307]\ttrain-rmse:0.039945\n",
      "[308]\ttrain-rmse:0.039716\n",
      "[309]\ttrain-rmse:0.039482\n",
      "[310]\ttrain-rmse:0.0393\n",
      "[311]\ttrain-rmse:0.039091\n",
      "[312]\ttrain-rmse:0.038934\n",
      "[313]\ttrain-rmse:0.038747\n",
      "[314]\ttrain-rmse:0.038584\n",
      "[315]\ttrain-rmse:0.038399\n",
      "[316]\ttrain-rmse:0.038209\n",
      "[317]\ttrain-rmse:0.038042\n",
      "[318]\ttrain-rmse:0.037877\n",
      "[319]\ttrain-rmse:0.037756\n",
      "[320]\ttrain-rmse:0.037596\n",
      "[321]\ttrain-rmse:0.037416\n",
      "[322]\ttrain-rmse:0.037231\n",
      "[323]\ttrain-rmse:0.037033\n",
      "[324]\ttrain-rmse:0.036848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[325]\ttrain-rmse:0.036715\n",
      "[326]\ttrain-rmse:0.036552\n",
      "[327]\ttrain-rmse:0.036394\n",
      "[328]\ttrain-rmse:0.036221\n",
      "[329]\ttrain-rmse:0.03607\n",
      "[330]\ttrain-rmse:0.0359\n",
      "[331]\ttrain-rmse:0.035759\n",
      "[332]\ttrain-rmse:0.035602\n",
      "[333]\ttrain-rmse:0.035458\n",
      "[334]\ttrain-rmse:0.035339\n",
      "[335]\ttrain-rmse:0.035181\n",
      "[336]\ttrain-rmse:0.035088\n",
      "[337]\ttrain-rmse:0.034947\n",
      "[338]\ttrain-rmse:0.03482\n",
      "[339]\ttrain-rmse:0.03469\n",
      "[340]\ttrain-rmse:0.034576\n",
      "[341]\ttrain-rmse:0.034467\n",
      "[342]\ttrain-rmse:0.034341\n",
      "[343]\ttrain-rmse:0.03419\n",
      "[344]\ttrain-rmse:0.034088\n",
      "[345]\ttrain-rmse:0.033984\n",
      "[346]\ttrain-rmse:0.03387\n",
      "[347]\ttrain-rmse:0.03377\n",
      "[348]\ttrain-rmse:0.033681\n",
      "[349]\ttrain-rmse:0.033582\n",
      "[350]\ttrain-rmse:0.03344\n",
      "[351]\ttrain-rmse:0.033338\n",
      "[352]\ttrain-rmse:0.033254\n",
      "[353]\ttrain-rmse:0.033163\n",
      "[354]\ttrain-rmse:0.033055\n",
      "[355]\ttrain-rmse:0.032965\n",
      "[356]\ttrain-rmse:0.032877\n",
      "[357]\ttrain-rmse:0.032782\n",
      "[358]\ttrain-rmse:0.032685\n",
      "[359]\ttrain-rmse:0.032582\n",
      "[360]\ttrain-rmse:0.032481\n",
      "[361]\ttrain-rmse:0.032354\n",
      "[362]\ttrain-rmse:0.03225\n",
      "[363]\ttrain-rmse:0.032138\n",
      "[364]\ttrain-rmse:0.032022\n",
      "[365]\ttrain-rmse:0.03193\n",
      "[366]\ttrain-rmse:0.031841\n",
      "[367]\ttrain-rmse:0.031767\n",
      "[368]\ttrain-rmse:0.031643\n",
      "[369]\ttrain-rmse:0.031562\n",
      "[370]\ttrain-rmse:0.031469\n",
      "[371]\ttrain-rmse:0.031385\n",
      "[372]\ttrain-rmse:0.031334\n",
      "[373]\ttrain-rmse:0.031253\n",
      "[374]\ttrain-rmse:0.031172\n",
      "[375]\ttrain-rmse:0.031084\n",
      "[376]\ttrain-rmse:0.030962\n",
      "[377]\ttrain-rmse:0.030887\n",
      "[378]\ttrain-rmse:0.030793\n",
      "[379]\ttrain-rmse:0.030726\n",
      "[380]\ttrain-rmse:0.030618\n",
      "[381]\ttrain-rmse:0.03054\n",
      "[382]\ttrain-rmse:0.03047\n",
      "[383]\ttrain-rmse:0.030415\n",
      "[384]\ttrain-rmse:0.030317\n",
      "[385]\ttrain-rmse:0.030257\n",
      "[386]\ttrain-rmse:0.030166\n",
      "[387]\ttrain-rmse:0.030035\n",
      "[388]\ttrain-rmse:0.029938\n",
      "[389]\ttrain-rmse:0.029843\n",
      "[390]\ttrain-rmse:0.029737\n",
      "[391]\ttrain-rmse:0.029655\n",
      "[392]\ttrain-rmse:0.0296\n",
      "[393]\ttrain-rmse:0.02954\n",
      "[394]\ttrain-rmse:0.029474\n",
      "[395]\ttrain-rmse:0.02942\n",
      "[396]\ttrain-rmse:0.029382\n",
      "[397]\ttrain-rmse:0.02929\n",
      "[398]\ttrain-rmse:0.029179\n",
      "[399]\ttrain-rmse:0.029114\n",
      "[400]\ttrain-rmse:0.029045\n",
      "[401]\ttrain-rmse:0.028987\n",
      "[402]\ttrain-rmse:0.028949\n",
      "[403]\ttrain-rmse:0.028878\n",
      "[404]\ttrain-rmse:0.0288\n",
      "[405]\ttrain-rmse:0.02875\n",
      "[406]\ttrain-rmse:0.028655\n",
      "[407]\ttrain-rmse:0.028621\n",
      "[408]\ttrain-rmse:0.028555\n",
      "[409]\ttrain-rmse:0.028497\n",
      "[410]\ttrain-rmse:0.028446\n",
      "[411]\ttrain-rmse:0.028362\n",
      "[412]\ttrain-rmse:0.028332\n",
      "[413]\ttrain-rmse:0.028275\n",
      "[414]\ttrain-rmse:0.028213\n",
      "[415]\ttrain-rmse:0.028132\n",
      "[416]\ttrain-rmse:0.02807\n",
      "[417]\ttrain-rmse:0.027996\n",
      "[418]\ttrain-rmse:0.027961\n",
      "[419]\ttrain-rmse:0.027895\n",
      "[420]\ttrain-rmse:0.027814\n",
      "[421]\ttrain-rmse:0.027741\n",
      "[422]\ttrain-rmse:0.027669\n",
      "[423]\ttrain-rmse:0.027624\n",
      "[424]\ttrain-rmse:0.027563\n",
      "[425]\ttrain-rmse:0.027514\n",
      "[426]\ttrain-rmse:0.027467\n",
      "[427]\ttrain-rmse:0.027395\n",
      "[428]\ttrain-rmse:0.027341\n",
      "[429]\ttrain-rmse:0.027251\n",
      "[430]\ttrain-rmse:0.027176\n",
      "[431]\ttrain-rmse:0.027133\n",
      "[432]\ttrain-rmse:0.027077\n",
      "[433]\ttrain-rmse:0.027023\n",
      "[434]\ttrain-rmse:0.02696\n",
      "[435]\ttrain-rmse:0.026885\n",
      "[436]\ttrain-rmse:0.026822\n",
      "[437]\ttrain-rmse:0.026778\n",
      "[438]\ttrain-rmse:0.026731\n",
      "[439]\ttrain-rmse:0.026646\n",
      "[440]\ttrain-rmse:0.0266\n",
      "[441]\ttrain-rmse:0.02655\n",
      "[442]\ttrain-rmse:0.026508\n",
      "[443]\ttrain-rmse:0.026437\n",
      "[444]\ttrain-rmse:0.026364\n",
      "[445]\ttrain-rmse:0.0263\n",
      "[446]\ttrain-rmse:0.026248\n",
      "[447]\ttrain-rmse:0.026188\n",
      "[448]\ttrain-rmse:0.026117\n",
      "[449]\ttrain-rmse:0.02609\n",
      "[450]\ttrain-rmse:0.02603\n",
      "[451]\ttrain-rmse:0.025969\n",
      "[452]\ttrain-rmse:0.025846\n",
      "[453]\ttrain-rmse:0.025796\n",
      "[454]\ttrain-rmse:0.025711\n",
      "[455]\ttrain-rmse:0.025644\n",
      "[456]\ttrain-rmse:0.025621\n",
      "[457]\ttrain-rmse:0.025543\n",
      "[458]\ttrain-rmse:0.025469\n",
      "[459]\ttrain-rmse:0.025424\n",
      "[460]\ttrain-rmse:0.025337\n",
      "[461]\ttrain-rmse:0.025281\n",
      "[462]\ttrain-rmse:0.025228\n",
      "[463]\ttrain-rmse:0.025165\n",
      "[464]\ttrain-rmse:0.025073\n",
      "[465]\ttrain-rmse:0.025019\n",
      "[466]\ttrain-rmse:0.024988\n",
      "[467]\ttrain-rmse:0.024932\n",
      "[468]\ttrain-rmse:0.024885\n",
      "[469]\ttrain-rmse:0.024818\n",
      "[470]\ttrain-rmse:0.024784\n",
      "[471]\ttrain-rmse:0.024705\n",
      "[472]\ttrain-rmse:0.024657\n",
      "[473]\ttrain-rmse:0.024615\n",
      "[474]\ttrain-rmse:0.024583\n",
      "[475]\ttrain-rmse:0.024547\n",
      "[476]\ttrain-rmse:0.024467\n",
      "[477]\ttrain-rmse:0.024373\n",
      "[478]\ttrain-rmse:0.024311\n",
      "[479]\ttrain-rmse:0.024281\n",
      "[480]\ttrain-rmse:0.024212\n",
      "[481]\ttrain-rmse:0.024159\n",
      "[482]\ttrain-rmse:0.024131\n",
      "[483]\ttrain-rmse:0.024076\n",
      "[484]\ttrain-rmse:0.023987\n",
      "[485]\ttrain-rmse:0.023925\n",
      "[486]\ttrain-rmse:0.023868\n",
      "[487]\ttrain-rmse:0.023847\n",
      "[488]\ttrain-rmse:0.02376\n",
      "[489]\ttrain-rmse:0.023744\n",
      "[490]\ttrain-rmse:0.023705\n",
      "[491]\ttrain-rmse:0.023626\n",
      "[492]\ttrain-rmse:0.023571\n",
      "[493]\ttrain-rmse:0.02351\n",
      "[494]\ttrain-rmse:0.023433\n",
      "[495]\ttrain-rmse:0.023382\n",
      "[496]\ttrain-rmse:0.023312\n",
      "[497]\ttrain-rmse:0.023247\n",
      "[498]\ttrain-rmse:0.023176\n",
      "[499]\ttrain-rmse:0.023133\n",
      "[500]\ttrain-rmse:0.023086\n",
      "[501]\ttrain-rmse:0.023048\n",
      "[502]\ttrain-rmse:0.022995\n",
      "[503]\ttrain-rmse:0.022932\n",
      "[504]\ttrain-rmse:0.022899\n",
      "[505]\ttrain-rmse:0.022826\n",
      "[506]\ttrain-rmse:0.022811\n",
      "[507]\ttrain-rmse:0.022751\n",
      "[508]\ttrain-rmse:0.022671\n",
      "[509]\ttrain-rmse:0.022632\n",
      "[510]\ttrain-rmse:0.022575\n",
      "[511]\ttrain-rmse:0.022519\n",
      "[512]\ttrain-rmse:0.022491\n",
      "[513]\ttrain-rmse:0.022457\n",
      "[514]\ttrain-rmse:0.022428\n",
      "[515]\ttrain-rmse:0.022366\n",
      "[516]\ttrain-rmse:0.022325\n",
      "[517]\ttrain-rmse:0.022276\n",
      "[518]\ttrain-rmse:0.022259\n",
      "[519]\ttrain-rmse:0.022197\n",
      "[520]\ttrain-rmse:0.022156\n",
      "[521]\ttrain-rmse:0.022128\n",
      "[522]\ttrain-rmse:0.022048\n",
      "[523]\ttrain-rmse:0.022003\n",
      "[524]\ttrain-rmse:0.021967\n",
      "[525]\ttrain-rmse:0.021893\n",
      "[526]\ttrain-rmse:0.021835\n",
      "[527]\ttrain-rmse:0.021782\n",
      "[528]\ttrain-rmse:0.021724\n",
      "[529]\ttrain-rmse:0.021667\n",
      "[530]\ttrain-rmse:0.021617\n",
      "[531]\ttrain-rmse:0.021589\n",
      "[532]\ttrain-rmse:0.021538\n",
      "[533]\ttrain-rmse:0.021498\n",
      "[534]\ttrain-rmse:0.021443\n",
      "[535]\ttrain-rmse:0.021421\n",
      "[536]\ttrain-rmse:0.021376\n",
      "[537]\ttrain-rmse:0.021347\n",
      "[538]\ttrain-rmse:0.021292\n",
      "[539]\ttrain-rmse:0.021213\n",
      "[540]\ttrain-rmse:0.021159\n",
      "[541]\ttrain-rmse:0.02114\n",
      "[542]\ttrain-rmse:0.021084\n",
      "[543]\ttrain-rmse:0.021051\n",
      "[544]\ttrain-rmse:0.021018\n",
      "[545]\ttrain-rmse:0.020985\n",
      "[546]\ttrain-rmse:0.02094\n",
      "[547]\ttrain-rmse:0.02089\n",
      "[548]\ttrain-rmse:0.020847\n",
      "[549]\ttrain-rmse:0.020773\n",
      "[550]\ttrain-rmse:0.020719\n",
      "[551]\ttrain-rmse:0.020673\n",
      "[552]\ttrain-rmse:0.020603\n",
      "[553]\ttrain-rmse:0.020567\n",
      "[554]\ttrain-rmse:0.020518\n",
      "[555]\ttrain-rmse:0.020476\n",
      "[556]\ttrain-rmse:0.020427\n",
      "[557]\ttrain-rmse:0.020379\n",
      "[558]\ttrain-rmse:0.020343\n",
      "[559]\ttrain-rmse:0.020276\n",
      "[560]\ttrain-rmse:0.020241\n",
      "[561]\ttrain-rmse:0.020208\n",
      "[562]\ttrain-rmse:0.020175\n",
      "[563]\ttrain-rmse:0.02012\n",
      "[564]\ttrain-rmse:0.020097\n",
      "[565]\ttrain-rmse:0.020025\n",
      "[566]\ttrain-rmse:0.019969\n",
      "[567]\ttrain-rmse:0.019916\n",
      "[568]\ttrain-rmse:0.019886\n",
      "[569]\ttrain-rmse:0.019822\n",
      "[570]\ttrain-rmse:0.019781\n",
      "[571]\ttrain-rmse:0.019707\n",
      "[572]\ttrain-rmse:0.019657\n",
      "[573]\ttrain-rmse:0.019621\n",
      "[574]\ttrain-rmse:0.01957\n",
      "[575]\ttrain-rmse:0.019507\n",
      "[576]\ttrain-rmse:0.019478\n",
      "[577]\ttrain-rmse:0.019421\n",
      "[578]\ttrain-rmse:0.019382\n",
      "[579]\ttrain-rmse:0.019319\n",
      "[580]\ttrain-rmse:0.019253\n",
      "[581]\ttrain-rmse:0.01922\n",
      "[582]\ttrain-rmse:0.019189\n",
      "[583]\ttrain-rmse:0.019147\n",
      "[584]\ttrain-rmse:0.019091\n",
      "[585]\ttrain-rmse:0.019054\n",
      "[586]\ttrain-rmse:0.019021\n",
      "[587]\ttrain-rmse:0.01898\n",
      "[588]\ttrain-rmse:0.018932\n",
      "[589]\ttrain-rmse:0.018899\n",
      "[590]\ttrain-rmse:0.01887\n",
      "[591]\ttrain-rmse:0.018826\n",
      "[592]\ttrain-rmse:0.018789\n",
      "[593]\ttrain-rmse:0.018733\n",
      "[594]\ttrain-rmse:0.018678\n",
      "[595]\ttrain-rmse:0.018637\n",
      "[596]\ttrain-rmse:0.01859\n",
      "[597]\ttrain-rmse:0.018576\n",
      "[598]\ttrain-rmse:0.018524\n",
      "[599]\ttrain-rmse:0.018465\n",
      "[600]\ttrain-rmse:0.018429\n",
      "[601]\ttrain-rmse:0.018391\n",
      "[602]\ttrain-rmse:0.018354\n",
      "[603]\ttrain-rmse:0.018323\n",
      "[604]\ttrain-rmse:0.01831\n",
      "[605]\ttrain-rmse:0.01828\n",
      "[606]\ttrain-rmse:0.018235\n",
      "[607]\ttrain-rmse:0.01818\n",
      "[608]\ttrain-rmse:0.018157\n",
      "[609]\ttrain-rmse:0.018149\n",
      "[610]\ttrain-rmse:0.018109\n",
      "[611]\ttrain-rmse:0.018065\n",
      "[612]\ttrain-rmse:0.01803\n",
      "[613]\ttrain-rmse:0.018004\n",
      "[614]\ttrain-rmse:0.017979\n",
      "[615]\ttrain-rmse:0.017958\n",
      "[616]\ttrain-rmse:0.017897\n",
      "[617]\ttrain-rmse:0.017861\n",
      "[618]\ttrain-rmse:0.017844\n",
      "[619]\ttrain-rmse:0.01781\n",
      "[620]\ttrain-rmse:0.017789\n",
      "[621]\ttrain-rmse:0.017753\n",
      "[622]\ttrain-rmse:0.017723\n",
      "[623]\ttrain-rmse:0.017665\n",
      "[624]\ttrain-rmse:0.017639\n",
      "[625]\ttrain-rmse:0.017583\n",
      "[626]\ttrain-rmse:0.017572\n",
      "[627]\ttrain-rmse:0.017526\n",
      "[628]\ttrain-rmse:0.017477\n",
      "[629]\ttrain-rmse:0.017448\n",
      "[630]\ttrain-rmse:0.017399\n",
      "[631]\ttrain-rmse:0.017356\n",
      "[632]\ttrain-rmse:0.017329\n",
      "[633]\ttrain-rmse:0.017273\n",
      "[634]\ttrain-rmse:0.017218\n",
      "[635]\ttrain-rmse:0.017186\n",
      "[636]\ttrain-rmse:0.017136\n",
      "[637]\ttrain-rmse:0.017103\n",
      "[638]\ttrain-rmse:0.017049\n",
      "[639]\ttrain-rmse:0.017032\n",
      "[640]\ttrain-rmse:0.017011\n",
      "[641]\ttrain-rmse:0.016982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[642]\ttrain-rmse:0.016963\n",
      "[643]\ttrain-rmse:0.016921\n",
      "[644]\ttrain-rmse:0.016862\n",
      "[645]\ttrain-rmse:0.016834\n",
      "[646]\ttrain-rmse:0.016794\n",
      "[647]\ttrain-rmse:0.016766\n",
      "[648]\ttrain-rmse:0.016752\n",
      "[649]\ttrain-rmse:0.016722\n",
      "[650]\ttrain-rmse:0.016681\n",
      "[651]\ttrain-rmse:0.016633\n",
      "[652]\ttrain-rmse:0.01658\n",
      "[653]\ttrain-rmse:0.016531\n",
      "[654]\ttrain-rmse:0.016514\n",
      "[655]\ttrain-rmse:0.01648\n",
      "[656]\ttrain-rmse:0.016452\n",
      "[657]\ttrain-rmse:0.0164\n",
      "[658]\ttrain-rmse:0.016355\n",
      "[659]\ttrain-rmse:0.016333\n",
      "[660]\ttrain-rmse:0.016304\n",
      "[661]\ttrain-rmse:0.016276\n",
      "[662]\ttrain-rmse:0.016226\n",
      "[663]\ttrain-rmse:0.016191\n",
      "[664]\ttrain-rmse:0.016151\n",
      "[665]\ttrain-rmse:0.016123\n",
      "[666]\ttrain-rmse:0.016087\n",
      "[667]\ttrain-rmse:0.016068\n",
      "[668]\ttrain-rmse:0.016038\n",
      "[669]\ttrain-rmse:0.016002\n",
      "[670]\ttrain-rmse:0.01597\n",
      "[671]\ttrain-rmse:0.015945\n",
      "[672]\ttrain-rmse:0.0159\n",
      "[673]\ttrain-rmse:0.015867\n",
      "[674]\ttrain-rmse:0.015842\n",
      "[675]\ttrain-rmse:0.01579\n",
      "[676]\ttrain-rmse:0.015744\n",
      "[677]\ttrain-rmse:0.015689\n",
      "[678]\ttrain-rmse:0.015662\n",
      "[679]\ttrain-rmse:0.015643\n",
      "[680]\ttrain-rmse:0.015592\n",
      "[681]\ttrain-rmse:0.015568\n",
      "[682]\ttrain-rmse:0.01554\n",
      "[683]\ttrain-rmse:0.015515\n",
      "[684]\ttrain-rmse:0.015481\n",
      "[685]\ttrain-rmse:0.015444\n",
      "[686]\ttrain-rmse:0.0154\n",
      "[687]\ttrain-rmse:0.015377\n",
      "[688]\ttrain-rmse:0.015346\n",
      "[689]\ttrain-rmse:0.015309\n",
      "[690]\ttrain-rmse:0.015257\n",
      "[691]\ttrain-rmse:0.015211\n",
      "[692]\ttrain-rmse:0.015194\n",
      "[693]\ttrain-rmse:0.015177\n",
      "[694]\ttrain-rmse:0.01513\n",
      "[695]\ttrain-rmse:0.015102\n",
      "[696]\ttrain-rmse:0.015077\n",
      "[697]\ttrain-rmse:0.01505\n",
      "[698]\ttrain-rmse:0.015013\n",
      "[699]\ttrain-rmse:0.014973\n",
      "[700]\ttrain-rmse:0.014942\n",
      "[701]\ttrain-rmse:0.014917\n",
      "[702]\ttrain-rmse:0.014897\n",
      "[703]\ttrain-rmse:0.01486\n",
      "[704]\ttrain-rmse:0.014825\n",
      "[705]\ttrain-rmse:0.014796\n",
      "[706]\ttrain-rmse:0.014776\n",
      "[707]\ttrain-rmse:0.014736\n",
      "[708]\ttrain-rmse:0.014709\n",
      "[709]\ttrain-rmse:0.014688\n",
      "[710]\ttrain-rmse:0.014665\n",
      "[711]\ttrain-rmse:0.014629\n",
      "[712]\ttrain-rmse:0.014596\n",
      "[713]\ttrain-rmse:0.014548\n",
      "[714]\ttrain-rmse:0.014523\n",
      "[715]\ttrain-rmse:0.014492\n",
      "[716]\ttrain-rmse:0.014457\n",
      "[717]\ttrain-rmse:0.014439\n",
      "[718]\ttrain-rmse:0.014417\n",
      "[719]\ttrain-rmse:0.014391\n",
      "[720]\ttrain-rmse:0.014345\n",
      "[721]\ttrain-rmse:0.014338\n",
      "[722]\ttrain-rmse:0.014298\n",
      "[723]\ttrain-rmse:0.014273\n",
      "[724]\ttrain-rmse:0.01424\n",
      "[725]\ttrain-rmse:0.014211\n",
      "[726]\ttrain-rmse:0.01418\n",
      "[727]\ttrain-rmse:0.014156\n",
      "[728]\ttrain-rmse:0.014136\n",
      "[729]\ttrain-rmse:0.01411\n",
      "[730]\ttrain-rmse:0.014078\n",
      "[731]\ttrain-rmse:0.014058\n",
      "[732]\ttrain-rmse:0.014037\n",
      "[733]\ttrain-rmse:0.013995\n",
      "[734]\ttrain-rmse:0.01398\n",
      "[735]\ttrain-rmse:0.013964\n",
      "[736]\ttrain-rmse:0.013931\n",
      "[737]\ttrain-rmse:0.013897\n",
      "[738]\ttrain-rmse:0.013881\n",
      "[739]\ttrain-rmse:0.013835\n",
      "[740]\ttrain-rmse:0.013805\n",
      "[741]\ttrain-rmse:0.0138\n",
      "[742]\ttrain-rmse:0.013774\n",
      "[743]\ttrain-rmse:0.013754\n",
      "[744]\ttrain-rmse:0.013712\n",
      "[745]\ttrain-rmse:0.013677\n",
      "[746]\ttrain-rmse:0.013653\n",
      "[747]\ttrain-rmse:0.013637\n",
      "[748]\ttrain-rmse:0.013603\n",
      "[749]\ttrain-rmse:0.013572\n",
      "[750]\ttrain-rmse:0.013543\n",
      "[751]\ttrain-rmse:0.013508\n",
      "[752]\ttrain-rmse:0.013491\n",
      "[753]\ttrain-rmse:0.013466\n",
      "[754]\ttrain-rmse:0.013432\n",
      "[755]\ttrain-rmse:0.013412\n",
      "[756]\ttrain-rmse:0.013386\n",
      "[757]\ttrain-rmse:0.013359\n",
      "[758]\ttrain-rmse:0.013334\n",
      "[759]\ttrain-rmse:0.013313\n",
      "[760]\ttrain-rmse:0.013294\n",
      "[761]\ttrain-rmse:0.013274\n",
      "[762]\ttrain-rmse:0.013265\n",
      "[763]\ttrain-rmse:0.013235\n",
      "[764]\ttrain-rmse:0.013213\n",
      "[765]\ttrain-rmse:0.01319\n",
      "[766]\ttrain-rmse:0.013169\n",
      "[767]\ttrain-rmse:0.013139\n",
      "[768]\ttrain-rmse:0.013115\n",
      "[769]\ttrain-rmse:0.013092\n",
      "[770]\ttrain-rmse:0.013062\n",
      "[771]\ttrain-rmse:0.013034\n",
      "[772]\ttrain-rmse:0.012999\n",
      "[773]\ttrain-rmse:0.012972\n",
      "[774]\ttrain-rmse:0.012947\n",
      "[775]\ttrain-rmse:0.012926\n",
      "[776]\ttrain-rmse:0.012903\n",
      "[777]\ttrain-rmse:0.012867\n",
      "[778]\ttrain-rmse:0.012838\n",
      "[779]\ttrain-rmse:0.012817\n",
      "[780]\ttrain-rmse:0.012787\n",
      "[781]\ttrain-rmse:0.012757\n",
      "[782]\ttrain-rmse:0.012718\n",
      "[783]\ttrain-rmse:0.012687\n",
      "[784]\ttrain-rmse:0.012666\n",
      "[785]\ttrain-rmse:0.012641\n",
      "[786]\ttrain-rmse:0.012611\n",
      "[787]\ttrain-rmse:0.012583\n",
      "[788]\ttrain-rmse:0.01256\n",
      "[789]\ttrain-rmse:0.012535\n",
      "[790]\ttrain-rmse:0.012518\n",
      "[791]\ttrain-rmse:0.012493\n",
      "[792]\ttrain-rmse:0.012475\n",
      "[793]\ttrain-rmse:0.012444\n",
      "[794]\ttrain-rmse:0.01242\n",
      "[795]\ttrain-rmse:0.012376\n",
      "[796]\ttrain-rmse:0.012352\n",
      "[797]\ttrain-rmse:0.01232\n",
      "[798]\ttrain-rmse:0.012305\n",
      "[799]\ttrain-rmse:0.012283\n",
      "[800]\ttrain-rmse:0.012247\n",
      "[801]\ttrain-rmse:0.01222\n",
      "[802]\ttrain-rmse:0.012201\n",
      "[803]\ttrain-rmse:0.01218\n",
      "[804]\ttrain-rmse:0.012158\n",
      "[805]\ttrain-rmse:0.01212\n",
      "[806]\ttrain-rmse:0.012103\n",
      "[807]\ttrain-rmse:0.012076\n",
      "[808]\ttrain-rmse:0.01205\n",
      "[809]\ttrain-rmse:0.012021\n",
      "[810]\ttrain-rmse:0.011998\n",
      "[811]\ttrain-rmse:0.011977\n",
      "[812]\ttrain-rmse:0.01195\n",
      "[813]\ttrain-rmse:0.011939\n",
      "[814]\ttrain-rmse:0.011925\n",
      "[815]\ttrain-rmse:0.011908\n",
      "[816]\ttrain-rmse:0.011888\n",
      "[817]\ttrain-rmse:0.011876\n",
      "[818]\ttrain-rmse:0.011857\n",
      "[819]\ttrain-rmse:0.011837\n",
      "[820]\ttrain-rmse:0.011796\n",
      "[821]\ttrain-rmse:0.011763\n",
      "[822]\ttrain-rmse:0.011732\n",
      "[823]\ttrain-rmse:0.011696\n",
      "[824]\ttrain-rmse:0.011665\n",
      "[825]\ttrain-rmse:0.011653\n",
      "[826]\ttrain-rmse:0.011615\n",
      "[827]\ttrain-rmse:0.011593\n",
      "[828]\ttrain-rmse:0.011565\n",
      "[829]\ttrain-rmse:0.011553\n",
      "[830]\ttrain-rmse:0.011524\n",
      "[831]\ttrain-rmse:0.011499\n",
      "[832]\ttrain-rmse:0.011481\n",
      "[833]\ttrain-rmse:0.011451\n",
      "[834]\ttrain-rmse:0.011427\n",
      "[835]\ttrain-rmse:0.011415\n",
      "[836]\ttrain-rmse:0.011383\n",
      "[837]\ttrain-rmse:0.011349\n",
      "[838]\ttrain-rmse:0.011335\n",
      "[839]\ttrain-rmse:0.011301\n",
      "[840]\ttrain-rmse:0.011262\n",
      "[841]\ttrain-rmse:0.011252\n",
      "[842]\ttrain-rmse:0.011228\n",
      "[843]\ttrain-rmse:0.011215\n",
      "[844]\ttrain-rmse:0.011197\n",
      "[845]\ttrain-rmse:0.011181\n",
      "[846]\ttrain-rmse:0.011165\n",
      "[847]\ttrain-rmse:0.01114\n",
      "[848]\ttrain-rmse:0.01111\n",
      "[849]\ttrain-rmse:0.011085\n",
      "[850]\ttrain-rmse:0.011073\n",
      "[851]\ttrain-rmse:0.011044\n",
      "[852]\ttrain-rmse:0.011014\n",
      "[853]\ttrain-rmse:0.010991\n",
      "[854]\ttrain-rmse:0.010967\n",
      "[855]\ttrain-rmse:0.010962\n",
      "[856]\ttrain-rmse:0.010947\n",
      "[857]\ttrain-rmse:0.010932\n",
      "[858]\ttrain-rmse:0.010919\n",
      "[859]\ttrain-rmse:0.010895\n",
      "[860]\ttrain-rmse:0.010876\n",
      "[861]\ttrain-rmse:0.01084\n",
      "[862]\ttrain-rmse:0.010816\n",
      "[863]\ttrain-rmse:0.010798\n",
      "[864]\ttrain-rmse:0.010784\n",
      "[865]\ttrain-rmse:0.01077\n",
      "[866]\ttrain-rmse:0.010735\n",
      "[867]\ttrain-rmse:0.010727\n",
      "[868]\ttrain-rmse:0.010702\n",
      "[869]\ttrain-rmse:0.010669\n",
      "[870]\ttrain-rmse:0.010647\n",
      "[871]\ttrain-rmse:0.010626\n",
      "[872]\ttrain-rmse:0.010602\n",
      "[873]\ttrain-rmse:0.010581\n",
      "[874]\ttrain-rmse:0.010573\n",
      "[875]\ttrain-rmse:0.010553\n",
      "[876]\ttrain-rmse:0.01054\n",
      "[877]\ttrain-rmse:0.01052\n",
      "[878]\ttrain-rmse:0.010498\n",
      "[879]\ttrain-rmse:0.010476\n",
      "[880]\ttrain-rmse:0.010447\n",
      "[881]\ttrain-rmse:0.010429\n",
      "[882]\ttrain-rmse:0.010418\n",
      "[883]\ttrain-rmse:0.010406\n",
      "[884]\ttrain-rmse:0.010391\n",
      "[885]\ttrain-rmse:0.010365\n",
      "[886]\ttrain-rmse:0.010335\n",
      "[887]\ttrain-rmse:0.010311\n",
      "[888]\ttrain-rmse:0.010294\n",
      "[889]\ttrain-rmse:0.010282\n",
      "[890]\ttrain-rmse:0.010269\n",
      "[891]\ttrain-rmse:0.010244\n",
      "[892]\ttrain-rmse:0.010225\n",
      "[893]\ttrain-rmse:0.010205\n",
      "[894]\ttrain-rmse:0.01019\n",
      "[895]\ttrain-rmse:0.010164\n",
      "[896]\ttrain-rmse:0.010153\n",
      "[897]\ttrain-rmse:0.010134\n",
      "[898]\ttrain-rmse:0.010105\n",
      "[899]\ttrain-rmse:0.010078\n",
      "[900]\ttrain-rmse:0.010061\n",
      "[901]\ttrain-rmse:0.010053\n",
      "[902]\ttrain-rmse:0.010043\n",
      "[903]\ttrain-rmse:0.010032\n",
      "[904]\ttrain-rmse:0.010006\n",
      "[905]\ttrain-rmse:0.009975\n",
      "[906]\ttrain-rmse:0.009949\n",
      "[907]\ttrain-rmse:0.009938\n",
      "[908]\ttrain-rmse:0.009918\n",
      "[909]\ttrain-rmse:0.00989\n",
      "[910]\ttrain-rmse:0.009874\n",
      "[911]\ttrain-rmse:0.009854\n",
      "[912]\ttrain-rmse:0.009844\n",
      "[913]\ttrain-rmse:0.009819\n",
      "[914]\ttrain-rmse:0.009784\n",
      "[915]\ttrain-rmse:0.009763\n",
      "[916]\ttrain-rmse:0.009744\n",
      "[917]\ttrain-rmse:0.009729\n",
      "[918]\ttrain-rmse:0.009705\n",
      "[919]\ttrain-rmse:0.009686\n",
      "[920]\ttrain-rmse:0.009669\n",
      "[921]\ttrain-rmse:0.009654\n",
      "[922]\ttrain-rmse:0.009633\n",
      "[923]\ttrain-rmse:0.009607\n",
      "[924]\ttrain-rmse:0.009583\n",
      "[925]\ttrain-rmse:0.009571\n",
      "[926]\ttrain-rmse:0.009561\n",
      "[927]\ttrain-rmse:0.009534\n",
      "[928]\ttrain-rmse:0.009521\n",
      "[929]\ttrain-rmse:0.009492\n",
      "[930]\ttrain-rmse:0.009476\n",
      "[931]\ttrain-rmse:0.009453\n",
      "[932]\ttrain-rmse:0.009434\n",
      "[933]\ttrain-rmse:0.009407\n",
      "[934]\ttrain-rmse:0.009395\n",
      "[935]\ttrain-rmse:0.009373\n",
      "[936]\ttrain-rmse:0.00936\n",
      "[937]\ttrain-rmse:0.009336\n",
      "[938]\ttrain-rmse:0.009329\n",
      "[939]\ttrain-rmse:0.009314\n",
      "[940]\ttrain-rmse:0.0093\n",
      "[941]\ttrain-rmse:0.009281\n",
      "[942]\ttrain-rmse:0.009266\n",
      "[943]\ttrain-rmse:0.009236\n",
      "[944]\ttrain-rmse:0.00921\n",
      "[945]\ttrain-rmse:0.009187\n",
      "[946]\ttrain-rmse:0.009178\n",
      "[947]\ttrain-rmse:0.009157\n",
      "[948]\ttrain-rmse:0.009144\n",
      "[949]\ttrain-rmse:0.009128\n",
      "[950]\ttrain-rmse:0.009122\n",
      "[951]\ttrain-rmse:0.009104\n",
      "[952]\ttrain-rmse:0.009074\n",
      "[953]\ttrain-rmse:0.009051\n",
      "[954]\ttrain-rmse:0.009032\n",
      "[955]\ttrain-rmse:0.009003\n",
      "[956]\ttrain-rmse:0.008987\n",
      "[957]\ttrain-rmse:0.008966\n",
      "[958]\ttrain-rmse:0.008947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[959]\ttrain-rmse:0.008944\n",
      "[960]\ttrain-rmse:0.008928\n",
      "[961]\ttrain-rmse:0.008912\n",
      "[962]\ttrain-rmse:0.008895\n",
      "[963]\ttrain-rmse:0.008871\n",
      "[964]\ttrain-rmse:0.008859\n",
      "[965]\ttrain-rmse:0.008846\n",
      "[966]\ttrain-rmse:0.008828\n",
      "[967]\ttrain-rmse:0.008803\n",
      "[968]\ttrain-rmse:0.008788\n",
      "[969]\ttrain-rmse:0.008767\n",
      "[970]\ttrain-rmse:0.008748\n",
      "[971]\ttrain-rmse:0.00873\n",
      "[972]\ttrain-rmse:0.008723\n",
      "[973]\ttrain-rmse:0.008709\n",
      "[974]\ttrain-rmse:0.008689\n",
      "[975]\ttrain-rmse:0.008673\n",
      "[976]\ttrain-rmse:0.008654\n",
      "[977]\ttrain-rmse:0.008637\n",
      "[978]\ttrain-rmse:0.008624\n",
      "[979]\ttrain-rmse:0.008604\n",
      "[980]\ttrain-rmse:0.008579\n",
      "[981]\ttrain-rmse:0.008566\n",
      "[982]\ttrain-rmse:0.008548\n",
      "[983]\ttrain-rmse:0.008532\n",
      "[984]\ttrain-rmse:0.00853\n",
      "[985]\ttrain-rmse:0.008516\n",
      "[986]\ttrain-rmse:0.008502\n",
      "[987]\ttrain-rmse:0.008486\n",
      "[988]\ttrain-rmse:0.008466\n",
      "[989]\ttrain-rmse:0.008441\n",
      "[990]\ttrain-rmse:0.008424\n",
      "[991]\ttrain-rmse:0.008405\n",
      "[992]\ttrain-rmse:0.008394\n",
      "[993]\ttrain-rmse:0.008371\n",
      "[994]\ttrain-rmse:0.008359\n",
      "[995]\ttrain-rmse:0.008339\n",
      "[996]\ttrain-rmse:0.008329\n",
      "[997]\ttrain-rmse:0.008321\n",
      "[998]\ttrain-rmse:0.008305\n",
      "[999]\ttrain-rmse:0.008287\n",
      "[1000]\ttrain-rmse:0.008279\n",
      "[1001]\ttrain-rmse:0.008266\n",
      "[1002]\ttrain-rmse:0.008252\n",
      "[1003]\ttrain-rmse:0.008236\n",
      "[1004]\ttrain-rmse:0.008226\n",
      "[1005]\ttrain-rmse:0.008217\n",
      "[1006]\ttrain-rmse:0.008201\n",
      "[1007]\ttrain-rmse:0.008182\n",
      "[1008]\ttrain-rmse:0.008168\n",
      "[1009]\ttrain-rmse:0.008145\n",
      "[1010]\ttrain-rmse:0.008131\n",
      "[1011]\ttrain-rmse:0.008115\n",
      "[1012]\ttrain-rmse:0.008113\n",
      "[1013]\ttrain-rmse:0.008105\n",
      "[1014]\ttrain-rmse:0.008089\n",
      "[1015]\ttrain-rmse:0.008082\n",
      "[1016]\ttrain-rmse:0.008065\n",
      "[1017]\ttrain-rmse:0.008048\n",
      "[1018]\ttrain-rmse:0.008031\n",
      "[1019]\ttrain-rmse:0.008022\n",
      "[1020]\ttrain-rmse:0.007994\n",
      "[1021]\ttrain-rmse:0.007977\n",
      "[1022]\ttrain-rmse:0.007967\n",
      "[1023]\ttrain-rmse:0.007956\n",
      "[1024]\ttrain-rmse:0.007944\n",
      "[1025]\ttrain-rmse:0.007926\n",
      "[1026]\ttrain-rmse:0.00791\n",
      "[1027]\ttrain-rmse:0.00789\n",
      "[1028]\ttrain-rmse:0.007874\n",
      "[1029]\ttrain-rmse:0.007854\n",
      "[1030]\ttrain-rmse:0.007836\n",
      "[1031]\ttrain-rmse:0.00783\n",
      "[1032]\ttrain-rmse:0.00782\n",
      "[1033]\ttrain-rmse:0.007806\n",
      "[1034]\ttrain-rmse:0.007796\n",
      "[1035]\ttrain-rmse:0.00778\n",
      "[1036]\ttrain-rmse:0.007762\n",
      "[1037]\ttrain-rmse:0.007746\n",
      "[1038]\ttrain-rmse:0.007739\n",
      "[1039]\ttrain-rmse:0.007721\n",
      "[1040]\ttrain-rmse:0.007707\n",
      "[1041]\ttrain-rmse:0.00769\n",
      "[1042]\ttrain-rmse:0.007669\n",
      "[1043]\ttrain-rmse:0.007663\n",
      "[1044]\ttrain-rmse:0.007653\n",
      "[1045]\ttrain-rmse:0.007636\n",
      "[1046]\ttrain-rmse:0.007614\n",
      "[1047]\ttrain-rmse:0.007606\n",
      "[1048]\ttrain-rmse:0.007592\n",
      "[1049]\ttrain-rmse:0.007578\n",
      "[1050]\ttrain-rmse:0.007567\n",
      "[1051]\ttrain-rmse:0.007551\n",
      "[1052]\ttrain-rmse:0.007539\n",
      "[1053]\ttrain-rmse:0.007534\n",
      "[1054]\ttrain-rmse:0.00752\n",
      "[1055]\ttrain-rmse:0.007512\n",
      "[1056]\ttrain-rmse:0.007495\n",
      "[1057]\ttrain-rmse:0.00748\n",
      "[1058]\ttrain-rmse:0.007468\n",
      "[1059]\ttrain-rmse:0.007451\n",
      "[1060]\ttrain-rmse:0.007443\n",
      "[1061]\ttrain-rmse:0.007424\n",
      "[1062]\ttrain-rmse:0.007409\n",
      "[1063]\ttrain-rmse:0.007397\n",
      "[1064]\ttrain-rmse:0.007378\n",
      "[1065]\ttrain-rmse:0.00737\n",
      "[1066]\ttrain-rmse:0.007353\n",
      "[1067]\ttrain-rmse:0.007334\n",
      "[1068]\ttrain-rmse:0.007325\n",
      "[1069]\ttrain-rmse:0.007312\n",
      "[1070]\ttrain-rmse:0.007298\n",
      "[1071]\ttrain-rmse:0.007282\n",
      "[1072]\ttrain-rmse:0.00727\n",
      "[1073]\ttrain-rmse:0.00726\n",
      "[1074]\ttrain-rmse:0.007246\n",
      "[1075]\ttrain-rmse:0.007238\n",
      "[1076]\ttrain-rmse:0.007231\n",
      "[1077]\ttrain-rmse:0.007218\n",
      "[1078]\ttrain-rmse:0.007201\n",
      "[1079]\ttrain-rmse:0.007188\n",
      "[1080]\ttrain-rmse:0.007176\n",
      "[1081]\ttrain-rmse:0.007167\n",
      "[1082]\ttrain-rmse:0.007151\n",
      "[1083]\ttrain-rmse:0.007139\n",
      "[1084]\ttrain-rmse:0.00713\n",
      "[1085]\ttrain-rmse:0.007113\n",
      "[1086]\ttrain-rmse:0.007096\n",
      "[1087]\ttrain-rmse:0.007087\n",
      "[1088]\ttrain-rmse:0.007082\n",
      "[1089]\ttrain-rmse:0.007073\n",
      "[1090]\ttrain-rmse:0.007059\n",
      "[1091]\ttrain-rmse:0.007043\n",
      "[1092]\ttrain-rmse:0.007034\n",
      "[1093]\ttrain-rmse:0.007021\n",
      "[1094]\ttrain-rmse:0.007002\n",
      "[1095]\ttrain-rmse:0.006997\n",
      "[1096]\ttrain-rmse:0.006984\n",
      "[1097]\ttrain-rmse:0.006968\n",
      "[1098]\ttrain-rmse:0.006958\n",
      "[1099]\ttrain-rmse:0.006947\n",
      "[1100]\ttrain-rmse:0.006934\n",
      "[1101]\ttrain-rmse:0.00692\n",
      "[1102]\ttrain-rmse:0.006917\n",
      "[1103]\ttrain-rmse:0.006904\n",
      "[1104]\ttrain-rmse:0.006881\n",
      "[1105]\ttrain-rmse:0.006864\n",
      "[1106]\ttrain-rmse:0.006852\n",
      "[1107]\ttrain-rmse:0.006838\n",
      "[1108]\ttrain-rmse:0.006828\n",
      "[1109]\ttrain-rmse:0.006815\n",
      "[1110]\ttrain-rmse:0.006805\n",
      "[1111]\ttrain-rmse:0.006789\n",
      "[1112]\ttrain-rmse:0.006779\n",
      "[1113]\ttrain-rmse:0.006773\n",
      "[1114]\ttrain-rmse:0.006759\n",
      "[1115]\ttrain-rmse:0.006754\n",
      "[1116]\ttrain-rmse:0.006734\n",
      "[1117]\ttrain-rmse:0.006724\n",
      "[1118]\ttrain-rmse:0.006714\n",
      "[1119]\ttrain-rmse:0.006696\n",
      "[1120]\ttrain-rmse:0.006682\n",
      "[1121]\ttrain-rmse:0.00667\n",
      "[1122]\ttrain-rmse:0.006656\n",
      "[1123]\ttrain-rmse:0.006646\n",
      "[1124]\ttrain-rmse:0.006633\n",
      "[1125]\ttrain-rmse:0.006618\n",
      "[1126]\ttrain-rmse:0.006599\n",
      "[1127]\ttrain-rmse:0.006583\n",
      "[1128]\ttrain-rmse:0.006571\n",
      "[1129]\ttrain-rmse:0.006556\n",
      "[1130]\ttrain-rmse:0.006542\n",
      "[1131]\ttrain-rmse:0.006535\n",
      "[1132]\ttrain-rmse:0.006527\n",
      "[1133]\ttrain-rmse:0.006509\n",
      "[1134]\ttrain-rmse:0.006499\n",
      "[1135]\ttrain-rmse:0.006489\n",
      "[1136]\ttrain-rmse:0.006478\n",
      "[1137]\ttrain-rmse:0.006466\n",
      "[1138]\ttrain-rmse:0.006453\n",
      "[1139]\ttrain-rmse:0.006437\n",
      "[1140]\ttrain-rmse:0.006432\n",
      "[1141]\ttrain-rmse:0.006421\n",
      "[1142]\ttrain-rmse:0.006407\n",
      "[1143]\ttrain-rmse:0.006401\n",
      "[1144]\ttrain-rmse:0.006391\n",
      "[1145]\ttrain-rmse:0.006376\n",
      "[1146]\ttrain-rmse:0.006369\n",
      "[1147]\ttrain-rmse:0.00636\n",
      "[1148]\ttrain-rmse:0.006355\n",
      "[1149]\ttrain-rmse:0.006351\n",
      "[1150]\ttrain-rmse:0.006335\n",
      "[1151]\ttrain-rmse:0.006326\n",
      "[1152]\ttrain-rmse:0.006319\n",
      "[1153]\ttrain-rmse:0.006311\n",
      "[1154]\ttrain-rmse:0.006305\n",
      "[1155]\ttrain-rmse:0.006295\n",
      "[1156]\ttrain-rmse:0.006283\n",
      "[1157]\ttrain-rmse:0.006274\n",
      "[1158]\ttrain-rmse:0.006264\n",
      "[1159]\ttrain-rmse:0.006253\n",
      "[1160]\ttrain-rmse:0.00624\n",
      "[1161]\ttrain-rmse:0.006235\n",
      "[1162]\ttrain-rmse:0.006222\n",
      "[1163]\ttrain-rmse:0.006209\n",
      "[1164]\ttrain-rmse:0.006199\n",
      "[1165]\ttrain-rmse:0.006186\n",
      "[1166]\ttrain-rmse:0.00618\n",
      "[1167]\ttrain-rmse:0.006164\n",
      "[1168]\ttrain-rmse:0.006154\n",
      "[1169]\ttrain-rmse:0.006136\n",
      "[1170]\ttrain-rmse:0.006128\n",
      "[1171]\ttrain-rmse:0.006121\n",
      "[1172]\ttrain-rmse:0.006113\n",
      "[1173]\ttrain-rmse:0.006107\n",
      "[1174]\ttrain-rmse:0.006092\n",
      "[1175]\ttrain-rmse:0.006076\n",
      "[1176]\ttrain-rmse:0.006062\n",
      "[1177]\ttrain-rmse:0.006054\n",
      "[1178]\ttrain-rmse:0.006039\n",
      "[1179]\ttrain-rmse:0.006023\n",
      "[1180]\ttrain-rmse:0.006015\n",
      "[1181]\ttrain-rmse:0.006008\n",
      "[1182]\ttrain-rmse:0.005996\n",
      "[1183]\ttrain-rmse:0.005984\n",
      "[1184]\ttrain-rmse:0.005971\n",
      "[1185]\ttrain-rmse:0.00596\n",
      "[1186]\ttrain-rmse:0.005945\n",
      "[1187]\ttrain-rmse:0.005931\n",
      "[1188]\ttrain-rmse:0.005916\n",
      "[1189]\ttrain-rmse:0.005905\n",
      "[1190]\ttrain-rmse:0.005892\n",
      "[1191]\ttrain-rmse:0.005882\n",
      "[1192]\ttrain-rmse:0.005867\n",
      "[1193]\ttrain-rmse:0.005859\n",
      "[1194]\ttrain-rmse:0.005848\n",
      "[1195]\ttrain-rmse:0.005843\n",
      "[1196]\ttrain-rmse:0.005836\n",
      "[1197]\ttrain-rmse:0.005825\n",
      "[1198]\ttrain-rmse:0.005809\n",
      "[1199]\ttrain-rmse:0.005797\n",
      "[1200]\ttrain-rmse:0.00579\n",
      "[1201]\ttrain-rmse:0.005784\n",
      "[1202]\ttrain-rmse:0.005769\n",
      "[1203]\ttrain-rmse:0.005761\n",
      "[1204]\ttrain-rmse:0.005753\n",
      "[1205]\ttrain-rmse:0.005737\n",
      "[1206]\ttrain-rmse:0.005728\n",
      "[1207]\ttrain-rmse:0.005721\n",
      "[1208]\ttrain-rmse:0.005707\n",
      "[1209]\ttrain-rmse:0.005698\n",
      "[1210]\ttrain-rmse:0.005685\n",
      "[1211]\ttrain-rmse:0.005676\n",
      "[1212]\ttrain-rmse:0.005667\n",
      "[1213]\ttrain-rmse:0.005657\n",
      "[1214]\ttrain-rmse:0.005646\n",
      "[1215]\ttrain-rmse:0.005624\n",
      "[1216]\ttrain-rmse:0.005612\n",
      "[1217]\ttrain-rmse:0.005608\n",
      "[1218]\ttrain-rmse:0.005595\n",
      "[1219]\ttrain-rmse:0.005587\n",
      "[1220]\ttrain-rmse:0.005577\n",
      "[1221]\ttrain-rmse:0.005569\n",
      "[1222]\ttrain-rmse:0.005561\n",
      "[1223]\ttrain-rmse:0.005547\n",
      "[1224]\ttrain-rmse:0.005528\n",
      "[1225]\ttrain-rmse:0.005515\n",
      "[1226]\ttrain-rmse:0.005504\n",
      "[1227]\ttrain-rmse:0.005498\n",
      "[1228]\ttrain-rmse:0.005488\n",
      "[1229]\ttrain-rmse:0.005476\n",
      "[1230]\ttrain-rmse:0.005465\n",
      "[1231]\ttrain-rmse:0.005454\n",
      "[1232]\ttrain-rmse:0.005445\n",
      "[1233]\ttrain-rmse:0.005431\n",
      "[1234]\ttrain-rmse:0.005425\n",
      "[1235]\ttrain-rmse:0.005417\n",
      "[1236]\ttrain-rmse:0.005416\n",
      "[1237]\ttrain-rmse:0.005409\n",
      "[1238]\ttrain-rmse:0.005399\n",
      "[1239]\ttrain-rmse:0.005391\n",
      "[1240]\ttrain-rmse:0.005382\n",
      "[1241]\ttrain-rmse:0.005368\n",
      "[1242]\ttrain-rmse:0.005359\n",
      "[1243]\ttrain-rmse:0.005349\n",
      "[1244]\ttrain-rmse:0.005339\n",
      "[1245]\ttrain-rmse:0.00533\n",
      "[1246]\ttrain-rmse:0.005323\n",
      "[1247]\ttrain-rmse:0.005315\n",
      "[1248]\ttrain-rmse:0.005307\n",
      "[1249]\ttrain-rmse:0.005298\n",
      "[1250]\ttrain-rmse:0.005289\n",
      "[1251]\ttrain-rmse:0.005274\n",
      "[1252]\ttrain-rmse:0.005268\n",
      "[1253]\ttrain-rmse:0.005259\n",
      "[1254]\ttrain-rmse:0.005249\n",
      "[1255]\ttrain-rmse:0.005238\n",
      "[1256]\ttrain-rmse:0.005226\n",
      "[1257]\ttrain-rmse:0.005223\n",
      "[1258]\ttrain-rmse:0.005215\n",
      "[1259]\ttrain-rmse:0.005204\n",
      "[1260]\ttrain-rmse:0.005194\n",
      "[1261]\ttrain-rmse:0.005183\n",
      "[1262]\ttrain-rmse:0.005175\n",
      "[1263]\ttrain-rmse:0.005171\n",
      "[1264]\ttrain-rmse:0.005157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1265]\ttrain-rmse:0.005144\n",
      "[1266]\ttrain-rmse:0.005137\n",
      "[1267]\ttrain-rmse:0.005131\n",
      "[1268]\ttrain-rmse:0.005119\n",
      "[1269]\ttrain-rmse:0.005103\n",
      "[1270]\ttrain-rmse:0.0051\n",
      "[1271]\ttrain-rmse:0.005092\n",
      "[1272]\ttrain-rmse:0.005084\n",
      "[1273]\ttrain-rmse:0.005073\n",
      "[1274]\ttrain-rmse:0.005069\n",
      "[1275]\ttrain-rmse:0.005061\n",
      "[1276]\ttrain-rmse:0.005051\n",
      "[1277]\ttrain-rmse:0.005045\n",
      "[1278]\ttrain-rmse:0.005038\n",
      "[1279]\ttrain-rmse:0.005036\n",
      "[1280]\ttrain-rmse:0.00503\n",
      "[1281]\ttrain-rmse:0.005022\n",
      "[1282]\ttrain-rmse:0.005007\n",
      "[1283]\ttrain-rmse:0.004997\n",
      "[1284]\ttrain-rmse:0.004989\n",
      "[1285]\ttrain-rmse:0.004977\n",
      "[1286]\ttrain-rmse:0.00497\n",
      "[1287]\ttrain-rmse:0.004962\n",
      "[1288]\ttrain-rmse:0.004952\n",
      "[1289]\ttrain-rmse:0.004945\n",
      "[1290]\ttrain-rmse:0.00494\n",
      "[1291]\ttrain-rmse:0.004932\n",
      "[1292]\ttrain-rmse:0.004931\n",
      "[1293]\ttrain-rmse:0.004922\n",
      "[1294]\ttrain-rmse:0.004917\n",
      "[1295]\ttrain-rmse:0.004906\n",
      "[1296]\ttrain-rmse:0.004899\n",
      "[1297]\ttrain-rmse:0.004883\n",
      "[1298]\ttrain-rmse:0.00488\n",
      "[1299]\ttrain-rmse:0.00487\n",
      "[1300]\ttrain-rmse:0.004859\n",
      "[1301]\ttrain-rmse:0.004854\n",
      "[1302]\ttrain-rmse:0.004842\n",
      "[1303]\ttrain-rmse:0.004835\n",
      "[1304]\ttrain-rmse:0.004825\n",
      "[1305]\ttrain-rmse:0.004814\n",
      "[1306]\ttrain-rmse:0.004806\n",
      "[1307]\ttrain-rmse:0.004796\n",
      "[1308]\ttrain-rmse:0.004792\n",
      "[1309]\ttrain-rmse:0.004777\n",
      "[1310]\ttrain-rmse:0.004774\n",
      "[1311]\ttrain-rmse:0.004762\n",
      "[1312]\ttrain-rmse:0.004748\n",
      "[1313]\ttrain-rmse:0.004737\n",
      "[1314]\ttrain-rmse:0.004733\n",
      "[1315]\ttrain-rmse:0.004731\n",
      "[1316]\ttrain-rmse:0.004725\n",
      "[1317]\ttrain-rmse:0.00472\n",
      "[1318]\ttrain-rmse:0.004715\n",
      "[1319]\ttrain-rmse:0.004711\n",
      "[1320]\ttrain-rmse:0.004703\n",
      "[1321]\ttrain-rmse:0.004694\n",
      "[1322]\ttrain-rmse:0.004685\n",
      "[1323]\ttrain-rmse:0.004672\n",
      "[1324]\ttrain-rmse:0.004665\n",
      "[1325]\ttrain-rmse:0.004655\n",
      "[1326]\ttrain-rmse:0.004649\n",
      "[1327]\ttrain-rmse:0.004641\n",
      "[1328]\ttrain-rmse:0.004631\n",
      "[1329]\ttrain-rmse:0.004621\n",
      "[1330]\ttrain-rmse:0.004608\n",
      "[1331]\ttrain-rmse:0.004601\n",
      "[1332]\ttrain-rmse:0.004595\n",
      "[1333]\ttrain-rmse:0.004592\n",
      "[1334]\ttrain-rmse:0.004584\n",
      "[1335]\ttrain-rmse:0.004571\n",
      "[1336]\ttrain-rmse:0.004567\n",
      "[1337]\ttrain-rmse:0.00456\n",
      "[1338]\ttrain-rmse:0.00455\n",
      "[1339]\ttrain-rmse:0.004542\n",
      "[1340]\ttrain-rmse:0.004539\n",
      "[1341]\ttrain-rmse:0.004528\n",
      "[1342]\ttrain-rmse:0.004516\n",
      "[1343]\ttrain-rmse:0.00451\n",
      "[1344]\ttrain-rmse:0.004503\n",
      "[1345]\ttrain-rmse:0.004493\n",
      "[1346]\ttrain-rmse:0.004489\n",
      "[1347]\ttrain-rmse:0.004482\n",
      "[1348]\ttrain-rmse:0.004469\n",
      "[1349]\ttrain-rmse:0.004463\n",
      "[1350]\ttrain-rmse:0.004457\n",
      "[1351]\ttrain-rmse:0.00445\n",
      "[1352]\ttrain-rmse:0.004442\n",
      "[1353]\ttrain-rmse:0.004434\n",
      "[1354]\ttrain-rmse:0.004428\n",
      "[1355]\ttrain-rmse:0.004421\n",
      "[1356]\ttrain-rmse:0.004414\n",
      "[1357]\ttrain-rmse:0.004406\n",
      "[1358]\ttrain-rmse:0.0044\n",
      "[1359]\ttrain-rmse:0.00439\n",
      "[1360]\ttrain-rmse:0.004379\n",
      "[1361]\ttrain-rmse:0.004369\n",
      "[1362]\ttrain-rmse:0.004363\n",
      "[1363]\ttrain-rmse:0.004359\n",
      "[1364]\ttrain-rmse:0.004351\n",
      "[1365]\ttrain-rmse:0.004341\n",
      "[1366]\ttrain-rmse:0.004329\n",
      "[1367]\ttrain-rmse:0.00432\n",
      "[1368]\ttrain-rmse:0.004311\n",
      "[1369]\ttrain-rmse:0.004301\n",
      "[1370]\ttrain-rmse:0.004292\n",
      "[1371]\ttrain-rmse:0.004285\n",
      "[1372]\ttrain-rmse:0.004276\n",
      "[1373]\ttrain-rmse:0.004271\n",
      "[1374]\ttrain-rmse:0.00426\n",
      "[1375]\ttrain-rmse:0.004249\n",
      "[1376]\ttrain-rmse:0.004239\n",
      "[1377]\ttrain-rmse:0.004228\n",
      "[1378]\ttrain-rmse:0.004218\n",
      "[1379]\ttrain-rmse:0.004212\n",
      "[1380]\ttrain-rmse:0.004206\n",
      "[1381]\ttrain-rmse:0.0042\n",
      "[1382]\ttrain-rmse:0.004191\n",
      "[1383]\ttrain-rmse:0.004183\n",
      "[1384]\ttrain-rmse:0.004177\n",
      "[1385]\ttrain-rmse:0.004173\n",
      "[1386]\ttrain-rmse:0.004167\n",
      "[1387]\ttrain-rmse:0.004159\n",
      "[1388]\ttrain-rmse:0.00415\n",
      "[1389]\ttrain-rmse:0.004145\n",
      "[1390]\ttrain-rmse:0.00414\n",
      "[1391]\ttrain-rmse:0.004133\n",
      "[1392]\ttrain-rmse:0.004128\n",
      "[1393]\ttrain-rmse:0.004121\n",
      "[1394]\ttrain-rmse:0.004119\n",
      "[1395]\ttrain-rmse:0.004115\n",
      "[1396]\ttrain-rmse:0.004106\n",
      "[1397]\ttrain-rmse:0.004099\n",
      "[1398]\ttrain-rmse:0.004089\n",
      "[1399]\ttrain-rmse:0.004084\n",
      "[1400]\ttrain-rmse:0.004078\n",
      "[1401]\ttrain-rmse:0.004071\n",
      "[1402]\ttrain-rmse:0.00407\n",
      "[1403]\ttrain-rmse:0.004063\n",
      "[1404]\ttrain-rmse:0.004057\n",
      "[1405]\ttrain-rmse:0.00405\n",
      "[1406]\ttrain-rmse:0.004046\n",
      "[1407]\ttrain-rmse:0.00404\n",
      "[1408]\ttrain-rmse:0.004035\n",
      "[1409]\ttrain-rmse:0.004025\n",
      "[1410]\ttrain-rmse:0.004018\n",
      "[1411]\ttrain-rmse:0.004012\n",
      "[1412]\ttrain-rmse:0.004003\n",
      "[1413]\ttrain-rmse:0.003999\n",
      "[1414]\ttrain-rmse:0.003991\n",
      "[1415]\ttrain-rmse:0.00398\n",
      "[1416]\ttrain-rmse:0.003974\n",
      "[1417]\ttrain-rmse:0.003967\n",
      "[1418]\ttrain-rmse:0.003962\n",
      "[1419]\ttrain-rmse:0.003955\n",
      "[1420]\ttrain-rmse:0.003948\n",
      "[1421]\ttrain-rmse:0.003942\n",
      "[1422]\ttrain-rmse:0.003934\n",
      "[1423]\ttrain-rmse:0.003926\n",
      "[1424]\ttrain-rmse:0.003916\n",
      "[1425]\ttrain-rmse:0.003912\n",
      "[1426]\ttrain-rmse:0.003904\n",
      "[1427]\ttrain-rmse:0.003899\n",
      "[1428]\ttrain-rmse:0.003889\n",
      "[1429]\ttrain-rmse:0.00388\n",
      "[1430]\ttrain-rmse:0.003869\n",
      "[1431]\ttrain-rmse:0.003863\n",
      "[1432]\ttrain-rmse:0.003856\n",
      "[1433]\ttrain-rmse:0.003846\n",
      "[1434]\ttrain-rmse:0.003842\n",
      "[1435]\ttrain-rmse:0.003835\n",
      "[1436]\ttrain-rmse:0.003832\n",
      "[1437]\ttrain-rmse:0.003827\n",
      "[1438]\ttrain-rmse:0.00382\n",
      "[1439]\ttrain-rmse:0.003814\n",
      "[1440]\ttrain-rmse:0.003807\n",
      "[1441]\ttrain-rmse:0.003801\n",
      "[1442]\ttrain-rmse:0.003795\n",
      "[1443]\ttrain-rmse:0.00379\n",
      "[1444]\ttrain-rmse:0.003787\n",
      "[1445]\ttrain-rmse:0.003778\n",
      "[1446]\ttrain-rmse:0.003774\n",
      "[1447]\ttrain-rmse:0.003768\n",
      "[1448]\ttrain-rmse:0.003763\n",
      "[1449]\ttrain-rmse:0.003757\n",
      "[1450]\ttrain-rmse:0.003748\n",
      "[1451]\ttrain-rmse:0.003744\n",
      "[1452]\ttrain-rmse:0.003737\n",
      "[1453]\ttrain-rmse:0.003731\n",
      "[1454]\ttrain-rmse:0.003727\n",
      "[1455]\ttrain-rmse:0.003719\n",
      "[1456]\ttrain-rmse:0.003714\n",
      "[1457]\ttrain-rmse:0.003705\n",
      "[1458]\ttrain-rmse:0.0037\n",
      "[1459]\ttrain-rmse:0.003695\n",
      "[1460]\ttrain-rmse:0.003684\n",
      "[1461]\ttrain-rmse:0.003677\n",
      "[1462]\ttrain-rmse:0.00367\n",
      "[1463]\ttrain-rmse:0.003666\n",
      "[1464]\ttrain-rmse:0.00366\n",
      "[1465]\ttrain-rmse:0.00365\n",
      "[1466]\ttrain-rmse:0.003649\n",
      "[1467]\ttrain-rmse:0.003638\n",
      "[1468]\ttrain-rmse:0.00363\n",
      "[1469]\ttrain-rmse:0.003627\n",
      "[1470]\ttrain-rmse:0.003622\n",
      "[1471]\ttrain-rmse:0.00362\n",
      "[1472]\ttrain-rmse:0.003611\n",
      "[1473]\ttrain-rmse:0.003598\n",
      "[1474]\ttrain-rmse:0.003591\n",
      "[1475]\ttrain-rmse:0.003586\n",
      "[1476]\ttrain-rmse:0.003578\n",
      "[1477]\ttrain-rmse:0.00357\n",
      "[1478]\ttrain-rmse:0.003561\n",
      "[1479]\ttrain-rmse:0.003555\n",
      "[1480]\ttrain-rmse:0.003552\n",
      "[1481]\ttrain-rmse:0.003547\n",
      "[1482]\ttrain-rmse:0.003546\n",
      "[1483]\ttrain-rmse:0.00354\n",
      "[1484]\ttrain-rmse:0.003535\n",
      "[1485]\ttrain-rmse:0.003526\n",
      "[1486]\ttrain-rmse:0.003523\n",
      "[1487]\ttrain-rmse:0.003521\n",
      "[1488]\ttrain-rmse:0.003513\n",
      "[1489]\ttrain-rmse:0.003506\n",
      "[1490]\ttrain-rmse:0.0035\n",
      "[1491]\ttrain-rmse:0.003494\n",
      "[1492]\ttrain-rmse:0.003488\n",
      "[1493]\ttrain-rmse:0.003484\n",
      "[1494]\ttrain-rmse:0.00348\n",
      "[1495]\ttrain-rmse:0.003471\n",
      "[1496]\ttrain-rmse:0.003465\n",
      "[1497]\ttrain-rmse:0.003461\n",
      "[1498]\ttrain-rmse:0.003455\n",
      "[1499]\ttrain-rmse:0.003451\n",
      "[1500]\ttrain-rmse:0.003447\n",
      "[1501]\ttrain-rmse:0.003442\n",
      "[1502]\ttrain-rmse:0.003436\n",
      "[1503]\ttrain-rmse:0.003432\n",
      "[1504]\ttrain-rmse:0.003425\n",
      "[1505]\ttrain-rmse:0.003422\n",
      "[1506]\ttrain-rmse:0.003415\n",
      "[1507]\ttrain-rmse:0.003409\n",
      "[1508]\ttrain-rmse:0.003401\n",
      "[1509]\ttrain-rmse:0.003395\n",
      "[1510]\ttrain-rmse:0.003389\n",
      "[1511]\ttrain-rmse:0.003385\n",
      "[1512]\ttrain-rmse:0.00338\n",
      "[1513]\ttrain-rmse:0.003376\n",
      "[1514]\ttrain-rmse:0.003371\n",
      "[1515]\ttrain-rmse:0.003366\n",
      "[1516]\ttrain-rmse:0.003363\n",
      "[1517]\ttrain-rmse:0.003357\n",
      "[1518]\ttrain-rmse:0.003353\n",
      "[1519]\ttrain-rmse:0.00335\n",
      "[1520]\ttrain-rmse:0.003344\n",
      "[1521]\ttrain-rmse:0.003336\n",
      "[1522]\ttrain-rmse:0.003327\n",
      "[1523]\ttrain-rmse:0.003319\n",
      "[1524]\ttrain-rmse:0.003313\n",
      "[1525]\ttrain-rmse:0.003308\n",
      "[1526]\ttrain-rmse:0.003301\n",
      "[1527]\ttrain-rmse:0.003293\n",
      "[1528]\ttrain-rmse:0.003289\n",
      "[1529]\ttrain-rmse:0.003286\n",
      "[1530]\ttrain-rmse:0.003274\n",
      "[1531]\ttrain-rmse:0.003267\n",
      "[1532]\ttrain-rmse:0.00326\n",
      "[1533]\ttrain-rmse:0.003254\n",
      "[1534]\ttrain-rmse:0.003246\n",
      "[1535]\ttrain-rmse:0.003239\n",
      "[1536]\ttrain-rmse:0.003232\n",
      "[1537]\ttrain-rmse:0.003226\n",
      "[1538]\ttrain-rmse:0.003221\n",
      "[1539]\ttrain-rmse:0.003218\n",
      "[1540]\ttrain-rmse:0.003212\n",
      "[1541]\ttrain-rmse:0.003209\n",
      "[1542]\ttrain-rmse:0.003202\n",
      "[1543]\ttrain-rmse:0.003197\n",
      "[1544]\ttrain-rmse:0.003191\n",
      "[1545]\ttrain-rmse:0.003185\n",
      "[1546]\ttrain-rmse:0.003182\n",
      "[1547]\ttrain-rmse:0.003176\n",
      "[1548]\ttrain-rmse:0.00317\n",
      "[1549]\ttrain-rmse:0.003165\n",
      "[1550]\ttrain-rmse:0.00316\n",
      "[1551]\ttrain-rmse:0.003156\n",
      "[1552]\ttrain-rmse:0.003151\n",
      "[1553]\ttrain-rmse:0.003147\n",
      "[1554]\ttrain-rmse:0.003146\n",
      "[1555]\ttrain-rmse:0.003142\n",
      "[1556]\ttrain-rmse:0.003137\n",
      "[1557]\ttrain-rmse:0.003129\n",
      "[1558]\ttrain-rmse:0.003125\n",
      "[1559]\ttrain-rmse:0.003119\n",
      "[1560]\ttrain-rmse:0.00311\n",
      "[1561]\ttrain-rmse:0.003107\n",
      "[1562]\ttrain-rmse:0.003099\n",
      "[1563]\ttrain-rmse:0.003095\n",
      "[1564]\ttrain-rmse:0.00309\n",
      "[1565]\ttrain-rmse:0.003088\n",
      "[1566]\ttrain-rmse:0.003085\n",
      "[1567]\ttrain-rmse:0.00308\n",
      "[1568]\ttrain-rmse:0.003076\n",
      "[1569]\ttrain-rmse:0.003069\n",
      "[1570]\ttrain-rmse:0.003061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1571]\ttrain-rmse:0.003056\n",
      "[1572]\ttrain-rmse:0.003052\n",
      "[1573]\ttrain-rmse:0.003047\n",
      "[1574]\ttrain-rmse:0.00304\n",
      "[1575]\ttrain-rmse:0.003037\n",
      "[1576]\ttrain-rmse:0.003033\n",
      "[1577]\ttrain-rmse:0.00303\n",
      "[1578]\ttrain-rmse:0.003025\n",
      "[1579]\ttrain-rmse:0.003019\n",
      "[1580]\ttrain-rmse:0.003016\n",
      "[1581]\ttrain-rmse:0.003012\n",
      "[1582]\ttrain-rmse:0.003009\n",
      "[1583]\ttrain-rmse:0.003005\n",
      "[1584]\ttrain-rmse:0.003\n",
      "[1585]\ttrain-rmse:0.002993\n",
      "[1586]\ttrain-rmse:0.002988\n",
      "[1587]\ttrain-rmse:0.00298\n",
      "[1588]\ttrain-rmse:0.002973\n",
      "[1589]\ttrain-rmse:0.002968\n",
      "[1590]\ttrain-rmse:0.002963\n",
      "[1591]\ttrain-rmse:0.002958\n",
      "[1592]\ttrain-rmse:0.002953\n",
      "[1593]\ttrain-rmse:0.002949\n",
      "[1594]\ttrain-rmse:0.002944\n",
      "[1595]\ttrain-rmse:0.002937\n",
      "[1596]\ttrain-rmse:0.002933\n",
      "[1597]\ttrain-rmse:0.002924\n",
      "[1598]\ttrain-rmse:0.002918\n",
      "[1599]\ttrain-rmse:0.002916\n",
      "[1600]\ttrain-rmse:0.002909\n",
      "[1601]\ttrain-rmse:0.002904\n",
      "[1602]\ttrain-rmse:0.002901\n",
      "[1603]\ttrain-rmse:0.002896\n",
      "[1604]\ttrain-rmse:0.002895\n",
      "[1605]\ttrain-rmse:0.002893\n",
      "[1606]\ttrain-rmse:0.002888\n",
      "[1607]\ttrain-rmse:0.002884\n",
      "[1608]\ttrain-rmse:0.002877\n",
      "[1609]\ttrain-rmse:0.002871\n",
      "[1610]\ttrain-rmse:0.002866\n",
      "[1611]\ttrain-rmse:0.002861\n",
      "[1612]\ttrain-rmse:0.002858\n",
      "[1613]\ttrain-rmse:0.002853\n",
      "[1614]\ttrain-rmse:0.002847\n",
      "[1615]\ttrain-rmse:0.002841\n",
      "[1616]\ttrain-rmse:0.002837\n",
      "[1617]\ttrain-rmse:0.002833\n",
      "[1618]\ttrain-rmse:0.002827\n",
      "[1619]\ttrain-rmse:0.002822\n",
      "[1620]\ttrain-rmse:0.002813\n",
      "[1621]\ttrain-rmse:0.002809\n",
      "[1622]\ttrain-rmse:0.002805\n",
      "[1623]\ttrain-rmse:0.002803\n",
      "[1624]\ttrain-rmse:0.002802\n",
      "[1625]\ttrain-rmse:0.002795\n",
      "[1626]\ttrain-rmse:0.00279\n",
      "[1627]\ttrain-rmse:0.002784\n",
      "[1628]\ttrain-rmse:0.00278\n",
      "[1629]\ttrain-rmse:0.002775\n",
      "[1630]\ttrain-rmse:0.002769\n",
      "[1631]\ttrain-rmse:0.002766\n",
      "[1632]\ttrain-rmse:0.002763\n",
      "[1633]\ttrain-rmse:0.002761\n",
      "[1634]\ttrain-rmse:0.002754\n",
      "[1635]\ttrain-rmse:0.002747\n",
      "[1636]\ttrain-rmse:0.002741\n",
      "[1637]\ttrain-rmse:0.002737\n",
      "[1638]\ttrain-rmse:0.002731\n",
      "[1639]\ttrain-rmse:0.002727\n",
      "[1640]\ttrain-rmse:0.002723\n",
      "[1641]\ttrain-rmse:0.002717\n",
      "[1642]\ttrain-rmse:0.002713\n",
      "[1643]\ttrain-rmse:0.002707\n",
      "[1644]\ttrain-rmse:0.0027\n",
      "[1645]\ttrain-rmse:0.002697\n",
      "[1646]\ttrain-rmse:0.00269\n",
      "[1647]\ttrain-rmse:0.002686\n",
      "[1648]\ttrain-rmse:0.00268\n",
      "[1649]\ttrain-rmse:0.002675\n",
      "[1650]\ttrain-rmse:0.002671\n",
      "[1651]\ttrain-rmse:0.002664\n",
      "[1652]\ttrain-rmse:0.002658\n",
      "[1653]\ttrain-rmse:0.002654\n",
      "[1654]\ttrain-rmse:0.002651\n",
      "[1655]\ttrain-rmse:0.002646\n",
      "[1656]\ttrain-rmse:0.002638\n",
      "[1657]\ttrain-rmse:0.002635\n",
      "[1658]\ttrain-rmse:0.002635\n",
      "[1659]\ttrain-rmse:0.002629\n",
      "[1660]\ttrain-rmse:0.002626\n",
      "[1661]\ttrain-rmse:0.002623\n",
      "[1662]\ttrain-rmse:0.00262\n",
      "[1663]\ttrain-rmse:0.002617\n",
      "[1664]\ttrain-rmse:0.002613\n",
      "[1665]\ttrain-rmse:0.002608\n",
      "[1666]\ttrain-rmse:0.002604\n",
      "[1667]\ttrain-rmse:0.0026\n",
      "[1668]\ttrain-rmse:0.002596\n",
      "[1669]\ttrain-rmse:0.00259\n",
      "[1670]\ttrain-rmse:0.002587\n",
      "[1671]\ttrain-rmse:0.002581\n",
      "[1672]\ttrain-rmse:0.002576\n",
      "[1673]\ttrain-rmse:0.002573\n",
      "[1674]\ttrain-rmse:0.002568\n",
      "[1675]\ttrain-rmse:0.002567\n",
      "[1676]\ttrain-rmse:0.002565\n",
      "[1677]\ttrain-rmse:0.002562\n",
      "[1678]\ttrain-rmse:0.002555\n",
      "[1679]\ttrain-rmse:0.002554\n",
      "[1680]\ttrain-rmse:0.002549\n",
      "[1681]\ttrain-rmse:0.002544\n",
      "[1682]\ttrain-rmse:0.002541\n",
      "[1683]\ttrain-rmse:0.002537\n",
      "[1684]\ttrain-rmse:0.002534\n",
      "[1685]\ttrain-rmse:0.00253\n",
      "[1686]\ttrain-rmse:0.002525\n",
      "[1687]\ttrain-rmse:0.002518\n",
      "[1688]\ttrain-rmse:0.002514\n",
      "[1689]\ttrain-rmse:0.002514\n",
      "[1690]\ttrain-rmse:0.00251\n",
      "[1691]\ttrain-rmse:0.002505\n",
      "[1692]\ttrain-rmse:0.002501\n",
      "[1693]\ttrain-rmse:0.002499\n",
      "[1694]\ttrain-rmse:0.002494\n",
      "[1695]\ttrain-rmse:0.002487\n",
      "[1696]\ttrain-rmse:0.00248\n",
      "[1697]\ttrain-rmse:0.002473\n",
      "[1698]\ttrain-rmse:0.002471\n",
      "[1699]\ttrain-rmse:0.002468\n",
      "[1700]\ttrain-rmse:0.002464\n",
      "[1701]\ttrain-rmse:0.00246\n",
      "[1702]\ttrain-rmse:0.002458\n",
      "[1703]\ttrain-rmse:0.002452\n",
      "[1704]\ttrain-rmse:0.002449\n",
      "[1705]\ttrain-rmse:0.002447\n",
      "[1706]\ttrain-rmse:0.002443\n",
      "[1707]\ttrain-rmse:0.00244\n",
      "[1708]\ttrain-rmse:0.002434\n",
      "[1709]\ttrain-rmse:0.002432\n",
      "[1710]\ttrain-rmse:0.002428\n",
      "[1711]\ttrain-rmse:0.002422\n",
      "[1712]\ttrain-rmse:0.002416\n",
      "[1713]\ttrain-rmse:0.002411\n",
      "[1714]\ttrain-rmse:0.002408\n",
      "[1715]\ttrain-rmse:0.002404\n",
      "[1716]\ttrain-rmse:0.002398\n",
      "[1717]\ttrain-rmse:0.002393\n",
      "[1718]\ttrain-rmse:0.002388\n",
      "[1719]\ttrain-rmse:0.002386\n",
      "[1720]\ttrain-rmse:0.002383\n",
      "[1721]\ttrain-rmse:0.002382\n",
      "[1722]\ttrain-rmse:0.00238\n",
      "[1723]\ttrain-rmse:0.002376\n",
      "[1724]\ttrain-rmse:0.002372\n",
      "[1725]\ttrain-rmse:0.002368\n",
      "[1726]\ttrain-rmse:0.002363\n",
      "[1727]\ttrain-rmse:0.002359\n",
      "[1728]\ttrain-rmse:0.002354\n",
      "[1729]\ttrain-rmse:0.002352\n",
      "[1730]\ttrain-rmse:0.002347\n",
      "[1731]\ttrain-rmse:0.002344\n",
      "[1732]\ttrain-rmse:0.002342\n",
      "[1733]\ttrain-rmse:0.002341\n",
      "[1734]\ttrain-rmse:0.002338\n",
      "[1735]\ttrain-rmse:0.002336\n",
      "[1736]\ttrain-rmse:0.002331\n",
      "[1737]\ttrain-rmse:0.002327\n",
      "[1738]\ttrain-rmse:0.002324\n",
      "[1739]\ttrain-rmse:0.002322\n",
      "[1740]\ttrain-rmse:0.002318\n",
      "[1741]\ttrain-rmse:0.002315\n",
      "[1742]\ttrain-rmse:0.002315\n",
      "[1743]\ttrain-rmse:0.002313\n",
      "[1744]\ttrain-rmse:0.002309\n",
      "[1745]\ttrain-rmse:0.002307\n",
      "[1746]\ttrain-rmse:0.002302\n",
      "[1747]\ttrain-rmse:0.002296\n",
      "[1748]\ttrain-rmse:0.002293\n",
      "[1749]\ttrain-rmse:0.002287\n",
      "[1750]\ttrain-rmse:0.002282\n",
      "[1751]\ttrain-rmse:0.00228\n",
      "[1752]\ttrain-rmse:0.002277\n",
      "[1753]\ttrain-rmse:0.002275\n",
      "[1754]\ttrain-rmse:0.00227\n",
      "[1755]\ttrain-rmse:0.002268\n",
      "[1756]\ttrain-rmse:0.002264\n",
      "[1757]\ttrain-rmse:0.00226\n",
      "[1758]\ttrain-rmse:0.002258\n",
      "[1759]\ttrain-rmse:0.002255\n",
      "[1760]\ttrain-rmse:0.002252\n",
      "[1761]\ttrain-rmse:0.002251\n",
      "[1762]\ttrain-rmse:0.002249\n",
      "[1763]\ttrain-rmse:0.002246\n",
      "[1764]\ttrain-rmse:0.002242\n",
      "[1765]\ttrain-rmse:0.002238\n",
      "[1766]\ttrain-rmse:0.002234\n",
      "[1767]\ttrain-rmse:0.002229\n",
      "[1768]\ttrain-rmse:0.002226\n",
      "[1769]\ttrain-rmse:0.002223\n",
      "[1770]\ttrain-rmse:0.002221\n",
      "[1771]\ttrain-rmse:0.002216\n",
      "[1772]\ttrain-rmse:0.002215\n",
      "[1773]\ttrain-rmse:0.002211\n",
      "[1774]\ttrain-rmse:0.002209\n",
      "[1775]\ttrain-rmse:0.002205\n",
      "[1776]\ttrain-rmse:0.002203\n",
      "[1777]\ttrain-rmse:0.002199\n",
      "[1778]\ttrain-rmse:0.002197\n",
      "[1779]\ttrain-rmse:0.002193\n",
      "[1780]\ttrain-rmse:0.002189\n",
      "[1781]\ttrain-rmse:0.002186\n",
      "[1782]\ttrain-rmse:0.002183\n",
      "[1783]\ttrain-rmse:0.002179\n",
      "[1784]\ttrain-rmse:0.002175\n",
      "[1785]\ttrain-rmse:0.002172\n",
      "[1786]\ttrain-rmse:0.002168\n",
      "[1787]\ttrain-rmse:0.002166\n",
      "[1788]\ttrain-rmse:0.00216\n",
      "[1789]\ttrain-rmse:0.002158\n",
      "[1790]\ttrain-rmse:0.002155\n",
      "[1791]\ttrain-rmse:0.002151\n",
      "[1792]\ttrain-rmse:0.002149\n",
      "[1793]\ttrain-rmse:0.002146\n",
      "[1794]\ttrain-rmse:0.002143\n",
      "[1795]\ttrain-rmse:0.00214\n",
      "[1796]\ttrain-rmse:0.002137\n",
      "[1797]\ttrain-rmse:0.002136\n",
      "[1798]\ttrain-rmse:0.00213\n",
      "[1799]\ttrain-rmse:0.002126\n",
      "[1800]\ttrain-rmse:0.002124\n",
      "[1801]\ttrain-rmse:0.002118\n",
      "[1802]\ttrain-rmse:0.002112\n",
      "[1803]\ttrain-rmse:0.002111\n",
      "[1804]\ttrain-rmse:0.002107\n",
      "[1805]\ttrain-rmse:0.002103\n",
      "[1806]\ttrain-rmse:0.002099\n",
      "[1807]\ttrain-rmse:0.002096\n",
      "[1808]\ttrain-rmse:0.002092\n",
      "[1809]\ttrain-rmse:0.00209\n",
      "[1810]\ttrain-rmse:0.002088\n",
      "[1811]\ttrain-rmse:0.002086\n",
      "[1812]\ttrain-rmse:0.002083\n",
      "[1813]\ttrain-rmse:0.002082\n",
      "[1814]\ttrain-rmse:0.002079\n",
      "[1815]\ttrain-rmse:0.002077\n",
      "[1816]\ttrain-rmse:0.002074\n",
      "[1817]\ttrain-rmse:0.002071\n",
      "[1818]\ttrain-rmse:0.002066\n",
      "[1819]\ttrain-rmse:0.002062\n",
      "[1820]\ttrain-rmse:0.002062\n",
      "[1821]\ttrain-rmse:0.00206\n",
      "[1822]\ttrain-rmse:0.002056\n",
      "[1823]\ttrain-rmse:0.002053\n",
      "[1824]\ttrain-rmse:0.00205\n",
      "[1825]\ttrain-rmse:0.002047\n",
      "[1826]\ttrain-rmse:0.002046\n",
      "[1827]\ttrain-rmse:0.002043\n",
      "[1828]\ttrain-rmse:0.002039\n",
      "[1829]\ttrain-rmse:0.002039\n",
      "[1830]\ttrain-rmse:0.002034\n",
      "[1831]\ttrain-rmse:0.002031\n",
      "[1832]\ttrain-rmse:0.002027\n",
      "[1833]\ttrain-rmse:0.002023\n",
      "[1834]\ttrain-rmse:0.002022\n",
      "[1835]\ttrain-rmse:0.00202\n",
      "[1836]\ttrain-rmse:0.002015\n",
      "[1837]\ttrain-rmse:0.002013\n",
      "[1838]\ttrain-rmse:0.00201\n",
      "[1839]\ttrain-rmse:0.002006\n",
      "[1840]\ttrain-rmse:0.002002\n",
      "[1841]\ttrain-rmse:0.001997\n",
      "[1842]\ttrain-rmse:0.001995\n",
      "[1843]\ttrain-rmse:0.001993\n",
      "[1844]\ttrain-rmse:0.001989\n",
      "[1845]\ttrain-rmse:0.001987\n",
      "[1846]\ttrain-rmse:0.001986\n",
      "[1847]\ttrain-rmse:0.001983\n",
      "[1848]\ttrain-rmse:0.001982\n",
      "[1849]\ttrain-rmse:0.001979\n",
      "[1850]\ttrain-rmse:0.001975\n",
      "[1851]\ttrain-rmse:0.001972\n",
      "[1852]\ttrain-rmse:0.00197\n",
      "[1853]\ttrain-rmse:0.001966\n",
      "[1854]\ttrain-rmse:0.001962\n",
      "[1855]\ttrain-rmse:0.001959\n",
      "[1856]\ttrain-rmse:0.001957\n",
      "[1857]\ttrain-rmse:0.001955\n",
      "[1858]\ttrain-rmse:0.001952\n",
      "[1859]\ttrain-rmse:0.001949\n",
      "[1860]\ttrain-rmse:0.001945\n",
      "[1861]\ttrain-rmse:0.00194\n",
      "[1862]\ttrain-rmse:0.001938\n",
      "[1863]\ttrain-rmse:0.001934\n",
      "[1864]\ttrain-rmse:0.001931\n",
      "[1865]\ttrain-rmse:0.001928\n",
      "[1866]\ttrain-rmse:0.001925\n",
      "[1867]\ttrain-rmse:0.001922\n",
      "[1868]\ttrain-rmse:0.001919\n",
      "[1869]\ttrain-rmse:0.001915\n",
      "[1870]\ttrain-rmse:0.001911\n",
      "[1871]\ttrain-rmse:0.001909\n",
      "[1872]\ttrain-rmse:0.001909\n",
      "[1873]\ttrain-rmse:0.001906\n",
      "[1874]\ttrain-rmse:0.001903\n",
      "[1875]\ttrain-rmse:0.001899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1876]\ttrain-rmse:0.001896\n",
      "[1877]\ttrain-rmse:0.001893\n",
      "[1878]\ttrain-rmse:0.001889\n",
      "[1879]\ttrain-rmse:0.001887\n",
      "[1880]\ttrain-rmse:0.001886\n",
      "[1881]\ttrain-rmse:0.001883\n",
      "[1882]\ttrain-rmse:0.001878\n",
      "[1883]\ttrain-rmse:0.001874\n",
      "[1884]\ttrain-rmse:0.001871\n",
      "[1885]\ttrain-rmse:0.001867\n",
      "[1886]\ttrain-rmse:0.001864\n",
      "[1887]\ttrain-rmse:0.001864\n",
      "[1888]\ttrain-rmse:0.001861\n",
      "[1889]\ttrain-rmse:0.001861\n",
      "[1890]\ttrain-rmse:0.001857\n",
      "[1891]\ttrain-rmse:0.001854\n",
      "[1892]\ttrain-rmse:0.001852\n",
      "[1893]\ttrain-rmse:0.00185\n",
      "[1894]\ttrain-rmse:0.001846\n",
      "[1895]\ttrain-rmse:0.001842\n",
      "[1896]\ttrain-rmse:0.001839\n",
      "[1897]\ttrain-rmse:0.001837\n",
      "[1898]\ttrain-rmse:0.001834\n",
      "[1899]\ttrain-rmse:0.00183\n",
      "[1900]\ttrain-rmse:0.001827\n",
      "[1901]\ttrain-rmse:0.001825\n",
      "[1902]\ttrain-rmse:0.001822\n",
      "[1903]\ttrain-rmse:0.001817\n",
      "[1904]\ttrain-rmse:0.001814\n",
      "[1905]\ttrain-rmse:0.001812\n",
      "[1906]\ttrain-rmse:0.001811\n",
      "[1907]\ttrain-rmse:0.001806\n",
      "[1908]\ttrain-rmse:0.001803\n",
      "[1909]\ttrain-rmse:0.0018\n",
      "[1910]\ttrain-rmse:0.001798\n",
      "[1911]\ttrain-rmse:0.001796\n",
      "[1912]\ttrain-rmse:0.001794\n",
      "[1913]\ttrain-rmse:0.001791\n",
      "[1914]\ttrain-rmse:0.001789\n",
      "[1915]\ttrain-rmse:0.001786\n",
      "[1916]\ttrain-rmse:0.001783\n",
      "[1917]\ttrain-rmse:0.00178\n",
      "[1918]\ttrain-rmse:0.001778\n",
      "[1919]\ttrain-rmse:0.001774\n",
      "[1920]\ttrain-rmse:0.001772\n",
      "[1921]\ttrain-rmse:0.00177\n",
      "[1922]\ttrain-rmse:0.001768\n",
      "[1923]\ttrain-rmse:0.001763\n",
      "[1924]\ttrain-rmse:0.001761\n",
      "[1925]\ttrain-rmse:0.001757\n",
      "[1926]\ttrain-rmse:0.001754\n",
      "[1927]\ttrain-rmse:0.001752\n",
      "[1928]\ttrain-rmse:0.001748\n",
      "[1929]\ttrain-rmse:0.001745\n",
      "[1930]\ttrain-rmse:0.001743\n",
      "[1931]\ttrain-rmse:0.00174\n",
      "[1932]\ttrain-rmse:0.001738\n",
      "[1933]\ttrain-rmse:0.001736\n",
      "[1934]\ttrain-rmse:0.001734\n",
      "[1935]\ttrain-rmse:0.00173\n",
      "[1936]\ttrain-rmse:0.001728\n",
      "[1937]\ttrain-rmse:0.001726\n",
      "[1938]\ttrain-rmse:0.001726\n",
      "[1939]\ttrain-rmse:0.001725\n",
      "[1940]\ttrain-rmse:0.001723\n",
      "[1941]\ttrain-rmse:0.001721\n",
      "[1942]\ttrain-rmse:0.00172\n",
      "[1943]\ttrain-rmse:0.001717\n",
      "[1944]\ttrain-rmse:0.001714\n",
      "[1945]\ttrain-rmse:0.001712\n",
      "[1946]\ttrain-rmse:0.001709\n",
      "[1947]\ttrain-rmse:0.001707\n",
      "[1948]\ttrain-rmse:0.001705\n",
      "[1949]\ttrain-rmse:0.001704\n",
      "[1950]\ttrain-rmse:0.0017\n",
      "[1951]\ttrain-rmse:0.001699\n",
      "[1952]\ttrain-rmse:0.001696\n",
      "[1953]\ttrain-rmse:0.001695\n",
      "[1954]\ttrain-rmse:0.001693\n",
      "[1955]\ttrain-rmse:0.001689\n",
      "[1956]\ttrain-rmse:0.001688\n",
      "[1957]\ttrain-rmse:0.001686\n",
      "[1958]\ttrain-rmse:0.001684\n",
      "[1959]\ttrain-rmse:0.001681\n",
      "[1960]\ttrain-rmse:0.001679\n",
      "[1961]\ttrain-rmse:0.001677\n",
      "[1962]\ttrain-rmse:0.001676\n",
      "[1963]\ttrain-rmse:0.001672\n",
      "[1964]\ttrain-rmse:0.001669\n",
      "[1965]\ttrain-rmse:0.001666\n",
      "[1966]\ttrain-rmse:0.001664\n",
      "[1967]\ttrain-rmse:0.001663\n",
      "[1968]\ttrain-rmse:0.001661\n",
      "[1969]\ttrain-rmse:0.001658\n",
      "[1970]\ttrain-rmse:0.001657\n",
      "[1971]\ttrain-rmse:0.001654\n",
      "[1972]\ttrain-rmse:0.00165\n",
      "[1973]\ttrain-rmse:0.001649\n",
      "[1974]\ttrain-rmse:0.001645\n",
      "[1975]\ttrain-rmse:0.001642\n",
      "[1976]\ttrain-rmse:0.00164\n",
      "[1977]\ttrain-rmse:0.001638\n",
      "[1978]\ttrain-rmse:0.001636\n",
      "[1979]\ttrain-rmse:0.001633\n",
      "[1980]\ttrain-rmse:0.001629\n",
      "[1981]\ttrain-rmse:0.001628\n",
      "[1982]\ttrain-rmse:0.001626\n",
      "[1983]\ttrain-rmse:0.001622\n",
      "[1984]\ttrain-rmse:0.001621\n",
      "[1985]\ttrain-rmse:0.001618\n",
      "[1986]\ttrain-rmse:0.001616\n",
      "[1987]\ttrain-rmse:0.001612\n",
      "[1988]\ttrain-rmse:0.001609\n",
      "[1989]\ttrain-rmse:0.001606\n",
      "[1990]\ttrain-rmse:0.001605\n",
      "[1991]\ttrain-rmse:0.001604\n",
      "[1992]\ttrain-rmse:0.001601\n",
      "[1993]\ttrain-rmse:0.0016\n",
      "[1994]\ttrain-rmse:0.001598\n",
      "[1995]\ttrain-rmse:0.001597\n",
      "[1996]\ttrain-rmse:0.001596\n",
      "[1997]\ttrain-rmse:0.001594\n",
      "[1998]\ttrain-rmse:0.001592\n",
      "[1999]\ttrain-rmse:0.001589\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'XGBoost' object has no attribute 'guess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-449830e4832e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/API/lib/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevallist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RMSE is '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/API/lib/models.py\u001b[0m in \u001b[0;36mrmse\u001b[0;34m(self, X_val, y_val)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mguessed_sales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguessed_sales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'XGBoost' object has no attribute 'guess'"
     ]
    }
   ],
   "source": [
    "m.fit(np.array(p.data[:900]),np.array(p.data_y[:900]),np.array(p.data[900:]),np.array(p.data_y[900:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model : the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 8 array(s), but instead got the following list of 10 arrays: [array([[1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-acff209d85b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/API/lib/models.py\u001b[0m in \u001b[0;36mguess\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mguess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_val_for_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1770\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1771\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1772\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/API/env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     86\u001b[0m                                  \u001b[0;34m'the following list of '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                                  \u001b[0;34m' arrays: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                                  '...')\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model : the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 8 array(s), but instead got the following list of 10 arrays: [array([[1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       ..."
     ]
    }
   ],
   "source": [
    "a= m.guess(np.array(p.data[900:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[ 4.5487549e+03, -2.0449758e-03,  5.1150273e+03,  5.4124844e+03,\n",
    "        5.8311650e+03,  5.9935474e+03,  6.0083091e+03,  7.0071553e+03,\n",
    "        4.5572519e-03,  5.3538613e+03,  5.0845806e+03,  4.5001465e+03,\n",
    "        4.3886338e+03,  3.8860903e+03,  4.0751150e+03,  1.2028217e-04,\n",
    "        5.1209390e+03,  5.5874277e+03,  5.4088252e+03,  5.2909756e+03,\n",
    "        5.7623330e+03,  5.5898267e+03, -1.7686486e-03,  5.6779883e+03,\n",
    "        4.3004800e+03,  3.7278887e+03,  3.8269873e+03,  3.8194197e+03,\n",
    "        4.8536758e+03, -3.8630962e-03,  5.0989370e+03,  4.8430942e+03,\n",
    "        5.1722686e+03,  5.2438677e+03,  5.9682578e+03,  6.9498062e+03,\n",
    "        7.1680546e-04,  4.8826611e+03,  4.8711289e+03,  4.4601250e+03,\n",
    "        5.3373564e+03,  4.3385029e-03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4409,    0, 5370, 5681, 5499, 6140, 6049, 7032,    0, 5970, 5633,\n",
       "       4709, 4601, 3725, 4055,    0, 5598, 5586, 5195, 5578, 5720, 5394,\n",
       "          0, 5182, 4127, 4044, 4008, 3900, 4717,    0, 4952, 4881, 4892,\n",
       "       5471, 5580, 7176,    0, 4997, 4486, 4327, 5530,    0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.data_y[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "500000< math.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt(data, lag=3):\n",
    "    df = pd.DataFrame(data)\n",
    "    columns = [df.shift(i) for i in range(1, lag+1)]\n",
    "    columns.append(df)\n",
    "    df = pd.concat(columns, axis=1)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4548.754900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4548.754900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.002045</td>\n",
       "      <td>4548.754900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5115.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5115.027300</td>\n",
       "      <td>-0.002045</td>\n",
       "      <td>4548.754900</td>\n",
       "      <td>5412.484400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5412.484400</td>\n",
       "      <td>5115.027300</td>\n",
       "      <td>-0.002045</td>\n",
       "      <td>5831.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5831.165000</td>\n",
       "      <td>5412.484400</td>\n",
       "      <td>5115.027300</td>\n",
       "      <td>5993.547400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5993.547400</td>\n",
       "      <td>5831.165000</td>\n",
       "      <td>5412.484400</td>\n",
       "      <td>6008.309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6008.309100</td>\n",
       "      <td>5993.547400</td>\n",
       "      <td>5831.165000</td>\n",
       "      <td>7007.155300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7007.155300</td>\n",
       "      <td>6008.309100</td>\n",
       "      <td>5993.547400</td>\n",
       "      <td>0.004557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.004557</td>\n",
       "      <td>7007.155300</td>\n",
       "      <td>6008.309100</td>\n",
       "      <td>5353.861300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5353.861300</td>\n",
       "      <td>0.004557</td>\n",
       "      <td>7007.155300</td>\n",
       "      <td>5084.580600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5084.580600</td>\n",
       "      <td>5353.861300</td>\n",
       "      <td>0.004557</td>\n",
       "      <td>4500.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4500.146500</td>\n",
       "      <td>5084.580600</td>\n",
       "      <td>5353.861300</td>\n",
       "      <td>4388.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4388.633800</td>\n",
       "      <td>4500.146500</td>\n",
       "      <td>5084.580600</td>\n",
       "      <td>3886.090300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3886.090300</td>\n",
       "      <td>4388.633800</td>\n",
       "      <td>4500.146500</td>\n",
       "      <td>4075.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4075.115000</td>\n",
       "      <td>3886.090300</td>\n",
       "      <td>4388.633800</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000120</td>\n",
       "      <td>4075.115000</td>\n",
       "      <td>3886.090300</td>\n",
       "      <td>5120.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5120.939000</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>4075.115000</td>\n",
       "      <td>5587.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5587.427700</td>\n",
       "      <td>5120.939000</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>5408.825200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5408.825200</td>\n",
       "      <td>5587.427700</td>\n",
       "      <td>5120.939000</td>\n",
       "      <td>5290.975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5290.975600</td>\n",
       "      <td>5408.825200</td>\n",
       "      <td>5587.427700</td>\n",
       "      <td>5762.333000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5762.333000</td>\n",
       "      <td>5290.975600</td>\n",
       "      <td>5408.825200</td>\n",
       "      <td>5589.826700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5589.826700</td>\n",
       "      <td>5762.333000</td>\n",
       "      <td>5290.975600</td>\n",
       "      <td>-0.001769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.001769</td>\n",
       "      <td>5589.826700</td>\n",
       "      <td>5762.333000</td>\n",
       "      <td>5677.988300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5677.988300</td>\n",
       "      <td>-0.001769</td>\n",
       "      <td>5589.826700</td>\n",
       "      <td>4300.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4300.480000</td>\n",
       "      <td>5677.988300</td>\n",
       "      <td>-0.001769</td>\n",
       "      <td>3727.888700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3727.888700</td>\n",
       "      <td>4300.480000</td>\n",
       "      <td>5677.988300</td>\n",
       "      <td>3826.987300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3826.987300</td>\n",
       "      <td>3727.888700</td>\n",
       "      <td>4300.480000</td>\n",
       "      <td>3819.419700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3819.419700</td>\n",
       "      <td>3826.987300</td>\n",
       "      <td>3727.888700</td>\n",
       "      <td>4853.675800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4853.675800</td>\n",
       "      <td>3819.419700</td>\n",
       "      <td>3826.987300</td>\n",
       "      <td>-0.003863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.003863</td>\n",
       "      <td>4853.675800</td>\n",
       "      <td>3819.419700</td>\n",
       "      <td>5098.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5098.937000</td>\n",
       "      <td>-0.003863</td>\n",
       "      <td>4853.675800</td>\n",
       "      <td>4843.094200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4843.094200</td>\n",
       "      <td>5098.937000</td>\n",
       "      <td>-0.003863</td>\n",
       "      <td>5172.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5172.268600</td>\n",
       "      <td>4843.094200</td>\n",
       "      <td>5098.937000</td>\n",
       "      <td>5243.867700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5243.867700</td>\n",
       "      <td>5172.268600</td>\n",
       "      <td>4843.094200</td>\n",
       "      <td>5968.257800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5968.257800</td>\n",
       "      <td>5243.867700</td>\n",
       "      <td>5172.268600</td>\n",
       "      <td>6949.806200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6949.806200</td>\n",
       "      <td>5968.257800</td>\n",
       "      <td>5243.867700</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000717</td>\n",
       "      <td>6949.806200</td>\n",
       "      <td>5968.257800</td>\n",
       "      <td>4882.661100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4882.661100</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>6949.806200</td>\n",
       "      <td>4871.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4871.128900</td>\n",
       "      <td>4882.661100</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>4460.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4460.125000</td>\n",
       "      <td>4871.128900</td>\n",
       "      <td>4882.661100</td>\n",
       "      <td>5337.356400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5337.356400</td>\n",
       "      <td>4460.125000</td>\n",
       "      <td>4871.128900</td>\n",
       "      <td>0.004339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0            0            0            0\n",
       "0      0.000000     0.000000     0.000000  4548.754900\n",
       "1   4548.754900     0.000000     0.000000    -0.002045\n",
       "2     -0.002045  4548.754900     0.000000  5115.027300\n",
       "3   5115.027300    -0.002045  4548.754900  5412.484400\n",
       "4   5412.484400  5115.027300    -0.002045  5831.165000\n",
       "5   5831.165000  5412.484400  5115.027300  5993.547400\n",
       "6   5993.547400  5831.165000  5412.484400  6008.309100\n",
       "7   6008.309100  5993.547400  5831.165000  7007.155300\n",
       "8   7007.155300  6008.309100  5993.547400     0.004557\n",
       "9      0.004557  7007.155300  6008.309100  5353.861300\n",
       "10  5353.861300     0.004557  7007.155300  5084.580600\n",
       "11  5084.580600  5353.861300     0.004557  4500.146500\n",
       "12  4500.146500  5084.580600  5353.861300  4388.633800\n",
       "13  4388.633800  4500.146500  5084.580600  3886.090300\n",
       "14  3886.090300  4388.633800  4500.146500  4075.115000\n",
       "15  4075.115000  3886.090300  4388.633800     0.000120\n",
       "16     0.000120  4075.115000  3886.090300  5120.939000\n",
       "17  5120.939000     0.000120  4075.115000  5587.427700\n",
       "18  5587.427700  5120.939000     0.000120  5408.825200\n",
       "19  5408.825200  5587.427700  5120.939000  5290.975600\n",
       "20  5290.975600  5408.825200  5587.427700  5762.333000\n",
       "21  5762.333000  5290.975600  5408.825200  5589.826700\n",
       "22  5589.826700  5762.333000  5290.975600    -0.001769\n",
       "23    -0.001769  5589.826700  5762.333000  5677.988300\n",
       "24  5677.988300    -0.001769  5589.826700  4300.480000\n",
       "25  4300.480000  5677.988300    -0.001769  3727.888700\n",
       "26  3727.888700  4300.480000  5677.988300  3826.987300\n",
       "27  3826.987300  3727.888700  4300.480000  3819.419700\n",
       "28  3819.419700  3826.987300  3727.888700  4853.675800\n",
       "29  4853.675800  3819.419700  3826.987300    -0.003863\n",
       "30    -0.003863  4853.675800  3819.419700  5098.937000\n",
       "31  5098.937000    -0.003863  4853.675800  4843.094200\n",
       "32  4843.094200  5098.937000    -0.003863  5172.268600\n",
       "33  5172.268600  4843.094200  5098.937000  5243.867700\n",
       "34  5243.867700  5172.268600  4843.094200  5968.257800\n",
       "35  5968.257800  5243.867700  5172.268600  6949.806200\n",
       "36  6949.806200  5968.257800  5243.867700     0.000717\n",
       "37     0.000717  6949.806200  5968.257800  4882.661100\n",
       "38  4882.661100     0.000717  6949.806200  4871.128900\n",
       "39  4871.128900  4882.661100     0.000717  4460.125000\n",
       "40  4460.125000  4871.128900  4882.661100  5337.356400\n",
       "41  5337.356400  4460.125000  4871.128900     0.004339"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ttt(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        b = dataset[i:(i+look_back)]\n",
    "        dataX.append(b)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "    return numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 2],\n",
       "        [2, 3]]), array([3, 4]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dataset([1,2,3,4],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_per(d, lockback=4):\n",
    "    return numpy.array([d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.reshape(x, (x.shape[0], 1, x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= numpy.reshape(x[0],(-1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2, 4, 6]]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 1s - loss: 17.6907\n",
      "Epoch 2/100\n",
      " - 0s - loss: 17.6476\n",
      "Epoch 3/100\n",
      " - 0s - loss: 17.6048\n",
      "Epoch 4/100\n",
      " - 0s - loss: 17.5624\n",
      "Epoch 5/100\n",
      " - 0s - loss: 17.5203\n",
      "Epoch 6/100\n",
      " - 0s - loss: 17.4802\n",
      "Epoch 7/100\n",
      " - 0s - loss: 17.4387\n",
      "Epoch 8/100\n",
      " - 0s - loss: 17.3976\n",
      "Epoch 9/100\n",
      " - 0s - loss: 17.3573\n",
      "Epoch 10/100\n",
      " - 0s - loss: 17.3168\n",
      "Epoch 11/100\n",
      " - 0s - loss: 17.2766\n",
      "Epoch 12/100\n",
      " - 0s - loss: 17.2368\n",
      "Epoch 13/100\n",
      " - 0s - loss: 17.1973\n",
      "Epoch 14/100\n",
      " - 0s - loss: 17.1581\n",
      "Epoch 15/100\n",
      " - 0s - loss: 17.1192\n",
      "Epoch 16/100\n",
      " - 0s - loss: 17.0806\n",
      "Epoch 17/100\n",
      " - 0s - loss: 17.0424\n",
      "Epoch 18/100\n",
      " - 0s - loss: 17.0044\n",
      "Epoch 19/100\n",
      " - 0s - loss: 16.9667\n",
      "Epoch 20/100\n",
      " - 0s - loss: 16.9290\n",
      "Epoch 21/100\n",
      " - 0s - loss: 16.8919\n",
      "Epoch 22/100\n",
      " - 0s - loss: 16.8551\n",
      "Epoch 23/100\n",
      " - 0s - loss: 16.8186\n",
      "Epoch 24/100\n",
      " - 0s - loss: 16.7823\n",
      "Epoch 25/100\n",
      " - 0s - loss: 16.7464\n",
      "Epoch 26/100\n",
      " - 0s - loss: 16.7106\n",
      "Epoch 27/100\n",
      " - 0s - loss: 16.6747\n",
      "Epoch 28/100\n",
      " - 0s - loss: 16.6391\n",
      "Epoch 29/100\n",
      " - 0s - loss: 16.6041\n",
      "Epoch 30/100\n",
      " - 0s - loss: 16.5694\n",
      "Epoch 31/100\n",
      " - 0s - loss: 16.5349\n",
      "Epoch 32/100\n",
      " - 0s - loss: 16.5005\n",
      "Epoch 33/100\n",
      " - 0s - loss: 16.4660\n",
      "Epoch 34/100\n",
      " - 0s - loss: 16.4321\n",
      "Epoch 35/100\n",
      " - 0s - loss: 16.3984\n",
      "Epoch 36/100\n",
      " - 0s - loss: 16.3649\n",
      "Epoch 37/100\n",
      " - 0s - loss: 16.3311\n",
      "Epoch 38/100\n",
      " - 0s - loss: 16.2979\n",
      "Epoch 39/100\n",
      " - 0s - loss: 16.2649\n",
      "Epoch 40/100\n",
      " - 0s - loss: 16.2319\n",
      "Epoch 41/100\n",
      " - 0s - loss: 16.1991\n",
      "Epoch 42/100\n",
      " - 0s - loss: 16.1661\n",
      "Epoch 43/100\n",
      " - 0s - loss: 16.1331\n",
      "Epoch 44/100\n",
      " - 0s - loss: 16.1006\n",
      "Epoch 45/100\n",
      " - 0s - loss: 16.0682\n",
      "Epoch 46/100\n",
      " - 0s - loss: 16.0358\n",
      "Epoch 47/100\n",
      " - 0s - loss: 16.0035\n",
      "Epoch 48/100\n",
      " - 0s - loss: 15.9713\n",
      "Epoch 49/100\n",
      " - 0s - loss: 15.9390\n",
      "Epoch 50/100\n",
      " - 0s - loss: 15.9068\n",
      "Epoch 51/100\n",
      " - 0s - loss: 15.8746\n",
      "Epoch 52/100\n",
      " - 0s - loss: 15.8424\n",
      "Epoch 53/100\n",
      " - 0s - loss: 15.8102\n",
      "Epoch 54/100\n",
      " - 0s - loss: 15.7779\n",
      "Epoch 55/100\n",
      " - 0s - loss: 15.7453\n",
      "Epoch 56/100\n",
      " - 0s - loss: 15.7129\n",
      "Epoch 57/100\n",
      " - 0s - loss: 15.6805\n",
      "Epoch 58/100\n",
      " - 0s - loss: 15.6480\n",
      "Epoch 59/100\n",
      " - 0s - loss: 15.6151\n",
      "Epoch 60/100\n",
      " - 0s - loss: 15.5824\n",
      "Epoch 61/100\n",
      " - 0s - loss: 15.5493\n",
      "Epoch 62/100\n",
      " - 0s - loss: 15.5164\n",
      "Epoch 63/100\n",
      " - 0s - loss: 15.4833\n",
      "Epoch 64/100\n",
      " - 0s - loss: 15.4500\n",
      "Epoch 65/100\n",
      " - 0s - loss: 15.4166\n",
      "Epoch 66/100\n",
      " - 0s - loss: 15.3831\n",
      "Epoch 67/100\n",
      " - 0s - loss: 15.3491\n",
      "Epoch 68/100\n",
      " - 0s - loss: 15.3152\n",
      "Epoch 69/100\n",
      " - 0s - loss: 15.2809\n",
      "Epoch 70/100\n",
      " - 0s - loss: 15.2466\n",
      "Epoch 71/100\n",
      " - 0s - loss: 15.2121\n",
      "Epoch 72/100\n",
      " - 0s - loss: 15.1771\n",
      "Epoch 73/100\n",
      " - 0s - loss: 15.1419\n",
      "Epoch 74/100\n",
      " - 0s - loss: 15.1067\n",
      "Epoch 75/100\n",
      " - 0s - loss: 15.0713\n",
      "Epoch 76/100\n",
      " - 0s - loss: 15.0357\n",
      "Epoch 77/100\n",
      " - 0s - loss: 14.9998\n",
      "Epoch 78/100\n",
      " - 0s - loss: 14.9636\n",
      "Epoch 79/100\n",
      " - 0s - loss: 14.9272\n",
      "Epoch 80/100\n",
      " - 0s - loss: 14.8906\n",
      "Epoch 81/100\n",
      " - 0s - loss: 14.8537\n",
      "Epoch 82/100\n",
      " - 0s - loss: 14.8165\n",
      "Epoch 83/100\n",
      " - 0s - loss: 14.7788\n",
      "Epoch 84/100\n",
      " - 0s - loss: 14.7411\n",
      "Epoch 85/100\n",
      " - 0s - loss: 14.7031\n",
      "Epoch 86/100\n",
      " - 0s - loss: 14.6647\n",
      "Epoch 87/100\n",
      " - 0s - loss: 14.6261\n",
      "Epoch 88/100\n",
      " - 0s - loss: 14.5874\n",
      "Epoch 89/100\n",
      " - 0s - loss: 14.5483\n",
      "Epoch 90/100\n",
      " - 0s - loss: 14.5090\n",
      "Epoch 91/100\n",
      " - 0s - loss: 14.4694\n",
      "Epoch 92/100\n",
      " - 0s - loss: 14.4295\n",
      "Epoch 93/100\n",
      " - 0s - loss: 14.3893\n",
      "Epoch 94/100\n",
      " - 0s - loss: 14.3487\n",
      "Epoch 95/100\n",
      " - 0s - loss: 14.3078\n",
      "Epoch 96/100\n",
      " - 0s - loss: 14.2667\n",
      "Epoch 97/100\n",
      " - 0s - loss: 14.2255\n",
      "Epoch 98/100\n",
      " - 0s - loss: 14.1840\n",
      "Epoch 99/100\n",
      " - 0s - loss: 14.1421\n",
      "Epoch 100/100\n",
      " - 0s - loss: 14.1001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x125e57518>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "look_back=3\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X, y, epochs=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-462d90135b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "np.concatenate(np.array([1,2,3]),np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3]+[4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([312., 528., 708., ...,  nan,  nan,  nan])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/labbrand/Desktop/API/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/labbrand/Desktop/API/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "m=time_series()\n",
    "project_name= 'forecast'\n",
    "m.build_model(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is training\n",
      "Train on 879 samples, validate on 21 samples\n",
      "Epoch 1/100\n",
      "Epoch 00001: val_loss improved from inf to 0.02869, saving model to ../projects/forecast/models/LSTM_weights.h5\n",
      " - 3s - loss: 0.1624 - val_loss: 0.0287\n",
      "Epoch 2/100\n",
      "Epoch 00002: val_loss improved from 0.02869 to 0.00291, saving model to ../projects/forecast/models/LSTM_weights.h5\n",
      " - 2s - loss: 0.0436 - val_loss: 0.0029\n",
      "Epoch 3/100\n",
      "Epoch 00003: val_loss improved from 0.00291 to 0.00189, saving model to ../projects/forecast/models/LSTM_weights.h5\n",
      " - 2s - loss: 0.0359 - val_loss: 0.0019\n",
      "Epoch 4/100\n",
      "Epoch 00004: val_loss improved from 0.00189 to 0.00143, saving model to ../projects/forecast/models/LSTM_weights.h5\n",
      " - 2s - loss: 0.0335 - val_loss: 0.0014\n",
      "Epoch 5/100\n",
      "Epoch 00005: val_loss improved from 0.00143 to 0.00112, saving model to ../projects/forecast/models/LSTM_weights.h5\n",
      " - 2s - loss: 0.0319 - val_loss: 0.0011\n",
      "Epoch 6/100\n",
      "Epoch 00006: val_loss improved from 0.00112 to 0.00059, saving model to ../projects/forecast/models/LSTM_weights.h5\n",
      " - 2s - loss: 0.0311 - val_loss: 5.8919e-04\n",
      "Epoch 7/100\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 2s - loss: 0.0303 - val_loss: 8.1702e-04\n",
      "Epoch 8/100\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 2s - loss: 0.0300 - val_loss: 0.0016\n",
      "Epoch 9/100\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 2s - loss: 0.0295 - val_loss: 9.7944e-04\n",
      "Epoch 10/100\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 2s - loss: 0.0288 - val_loss: 0.0078\n",
      "Epoch 11/100\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 2s - loss: 0.0279 - val_loss: 0.0018\n",
      "Epoch 12/100\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 2s - loss: 0.0275 - val_loss: 0.0030\n",
      "Epoch 13/100\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 2s - loss: 0.0275 - val_loss: 0.0011\n",
      "Epoch 14/100\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 2s - loss: 0.0269 - val_loss: 9.2709e-04\n",
      "Epoch 15/100\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 2s - loss: 0.0267 - val_loss: 5.9988e-04\n",
      "Epoch 16/100\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 2s - loss: 0.0265 - val_loss: 0.0013\n",
      "Epoch 17/100\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 2s - loss: 0.0261 - val_loss: 6.8646e-04\n",
      "Epoch 18/100\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 2s - loss: 0.0258 - val_loss: 0.0018\n",
      "Epoch 19/100\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 2s - loss: 0.0255 - val_loss: 7.5250e-04\n",
      "Epoch 20/100\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 2s - loss: 0.0253 - val_loss: 0.0013\n",
      "Epoch 21/100\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 2s - loss: 0.0251 - val_loss: 8.6694e-04\n",
      "Epoch 22/100\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 2s - loss: 0.0247 - val_loss: 0.0018\n",
      "Epoch 23/100\n",
      "Epoch 00023: val_loss did not improve\n",
      " - 2s - loss: 0.0244 - val_loss: 7.5211e-04\n",
      "Epoch 24/100\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 2s - loss: 0.0244 - val_loss: 7.6043e-04\n",
      "Epoch 25/100\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 2s - loss: 0.0236 - val_loss: 0.0037\n",
      "Epoch 26/100\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 2s - loss: 0.0239 - val_loss: 0.0086\n",
      "Epoch 27/100\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 2s - loss: 0.0240 - val_loss: 0.0029\n",
      "Epoch 28/100\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 2s - loss: 0.0235 - val_loss: 0.0012\n",
      "Epoch 29/100\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 2s - loss: 0.0235 - val_loss: 0.0052\n",
      "Epoch 30/100\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 2s - loss: 0.0237 - val_loss: 7.7853e-04\n",
      "Epoch 31/100\n",
      "Epoch 00031: val_loss did not improve\n",
      " - 2s - loss: 0.0235 - val_loss: 0.0025\n",
      "Epoch 32/100\n",
      "Epoch 00032: val_loss did not improve\n",
      " - 2s - loss: 0.0230 - val_loss: 0.0078\n",
      "Epoch 33/100\n",
      "Epoch 00033: val_loss did not improve\n",
      " - 2s - loss: 0.0229 - val_loss: 0.0024\n",
      "Epoch 34/100\n",
      "Epoch 00034: val_loss did not improve\n",
      " - 2s - loss: 0.0224 - val_loss: 0.0016\n",
      "Epoch 35/100\n",
      "Epoch 00035: val_loss did not improve\n",
      " - 2s - loss: 0.0227 - val_loss: 0.0015\n",
      "Epoch 36/100\n",
      "Epoch 00036: val_loss did not improve\n",
      " - 2s - loss: 0.0228 - val_loss: 0.0025\n",
      "Epoch 37/100\n",
      "Epoch 00037: val_loss did not improve\n",
      " - 2s - loss: 0.0227 - val_loss: 0.0019\n",
      "Epoch 38/100\n",
      "Epoch 00038: val_loss did not improve\n",
      " - 2s - loss: 0.0222 - val_loss: 9.1538e-04\n",
      "Epoch 39/100\n",
      "Epoch 00039: val_loss did not improve\n",
      " - 2s - loss: 0.0222 - val_loss: 0.0011\n",
      "Epoch 40/100\n",
      "Epoch 00040: val_loss did not improve\n",
      " - 2s - loss: 0.0223 - val_loss: 0.0013\n",
      "Epoch 41/100\n",
      "Epoch 00041: val_loss improved from 0.00059 to 0.00051, saving model to ../projects/forecast/models/LSTM_weights.h5\n",
      " - 2s - loss: 0.0217 - val_loss: 5.1083e-04\n",
      "Epoch 42/100\n",
      "Epoch 00042: val_loss did not improve\n",
      " - 2s - loss: 0.0222 - val_loss: 5.6760e-04\n",
      "Epoch 43/100\n",
      "Epoch 00043: val_loss did not improve\n",
      " - 2s - loss: 0.0219 - val_loss: 0.0011\n",
      "Epoch 44/100\n",
      "Epoch 00044: val_loss did not improve\n",
      " - 2s - loss: 0.0220 - val_loss: 0.0013\n",
      "Epoch 45/100\n",
      "Epoch 00045: val_loss did not improve\n",
      " - 2s - loss: 0.0214 - val_loss: 0.0012\n",
      "Epoch 46/100\n",
      "Epoch 00046: val_loss did not improve\n",
      " - 2s - loss: 0.0221 - val_loss: 0.0025\n",
      "Epoch 47/100\n",
      "Epoch 00047: val_loss did not improve\n",
      " - 2s - loss: 0.0215 - val_loss: 0.0015\n",
      "Epoch 48/100\n",
      "Epoch 00048: val_loss did not improve\n",
      " - 2s - loss: 0.0215 - val_loss: 0.0011\n",
      "Epoch 49/100\n",
      "Epoch 00049: val_loss did not improve\n",
      " - 3s - loss: 0.0217 - val_loss: 0.0018\n",
      "Epoch 50/100\n",
      "Epoch 00050: val_loss did not improve\n",
      " - 2s - loss: 0.0213 - val_loss: 0.0012\n",
      "Epoch 51/100\n",
      "Epoch 00051: val_loss did not improve\n",
      " - 2s - loss: 0.0214 - val_loss: 0.0015\n",
      "Epoch 52/100\n",
      "Epoch 00052: val_loss did not improve\n",
      " - 2s - loss: 0.0214 - val_loss: 0.0012\n",
      "Epoch 53/100\n",
      "Epoch 00053: val_loss did not improve\n",
      " - 2s - loss: 0.0212 - val_loss: 9.9188e-04\n",
      "Epoch 54/100\n",
      "Epoch 00054: val_loss did not improve\n",
      " - 2s - loss: 0.0209 - val_loss: 6.6635e-04\n",
      "Epoch 55/100\n",
      "Epoch 00055: val_loss did not improve\n",
      " - 2s - loss: 0.0211 - val_loss: 0.0010\n",
      "Epoch 56/100\n",
      "Epoch 00056: val_loss did not improve\n",
      " - 2s - loss: 0.0210 - val_loss: 0.0022\n",
      "Epoch 57/100\n",
      "Epoch 00057: val_loss did not improve\n",
      " - 2s - loss: 0.0212 - val_loss: 0.0014\n",
      "Epoch 58/100\n",
      "Epoch 00058: val_loss did not improve\n",
      " - 2s - loss: 0.0208 - val_loss: 7.4503e-04\n",
      "Epoch 59/100\n",
      "Epoch 00059: val_loss did not improve\n",
      " - 2s - loss: 0.0208 - val_loss: 0.0023\n",
      "Epoch 60/100\n",
      "Epoch 00060: val_loss did not improve\n",
      " - 2s - loss: 0.0207 - val_loss: 0.0012\n",
      "Epoch 61/100\n",
      "Epoch 00061: val_loss did not improve\n",
      " - 2s - loss: 0.0206 - val_loss: 0.0015\n",
      "Epoch 62/100\n",
      "Epoch 00062: val_loss did not improve\n",
      " - 2s - loss: 0.0203 - val_loss: 0.0013\n",
      "Epoch 63/100\n",
      "Epoch 00063: val_loss did not improve\n",
      " - 2s - loss: 0.0202 - val_loss: 8.6280e-04\n",
      "Epoch 64/100\n",
      "Epoch 00064: val_loss did not improve\n",
      " - 2s - loss: 0.0199 - val_loss: 0.0021\n",
      "Epoch 65/100\n",
      "Epoch 00065: val_loss did not improve\n",
      " - 2s - loss: 0.0203 - val_loss: 7.4819e-04\n",
      "Epoch 66/100\n",
      "Epoch 00066: val_loss did not improve\n",
      " - 2s - loss: 0.0202 - val_loss: 0.0047\n",
      "Epoch 67/100\n",
      "Epoch 00067: val_loss did not improve\n",
      " - 2s - loss: 0.0204 - val_loss: 0.0044\n",
      "Epoch 68/100\n",
      "Epoch 00068: val_loss did not improve\n",
      " - 2s - loss: 0.0201 - val_loss: 8.4547e-04\n",
      "Epoch 69/100\n",
      "Epoch 00069: val_loss did not improve\n",
      " - 2s - loss: 0.0201 - val_loss: 0.0010\n",
      "Epoch 70/100\n",
      "Epoch 00070: val_loss did not improve\n",
      " - 2s - loss: 0.0200 - val_loss: 0.0012\n",
      "Epoch 71/100\n",
      "Epoch 00071: val_loss did not improve\n",
      " - 2s - loss: 0.0204 - val_loss: 5.8027e-04\n",
      "Epoch 72/100\n",
      "Epoch 00072: val_loss did not improve\n",
      " - 2s - loss: 0.0198 - val_loss: 0.0013\n",
      "Epoch 73/100\n",
      "Epoch 00073: val_loss did not improve\n",
      " - 2s - loss: 0.0199 - val_loss: 0.0012\n",
      "Epoch 74/100\n",
      "Epoch 00074: val_loss did not improve\n",
      " - 2s - loss: 0.0199 - val_loss: 0.0016\n",
      "Epoch 75/100\n",
      "Epoch 00075: val_loss did not improve\n",
      " - 2s - loss: 0.0200 - val_loss: 0.0011\n",
      "Epoch 76/100\n",
      "Epoch 00076: val_loss did not improve\n",
      " - 2s - loss: 0.0196 - val_loss: 0.0014\n",
      "Epoch 77/100\n",
      "Epoch 00077: val_loss did not improve\n",
      " - 2s - loss: 0.0193 - val_loss: 0.0018\n",
      "Epoch 78/100\n",
      "Epoch 00078: val_loss improved from 0.00051 to 0.00047, saving model to ../projects/forecast/models/LSTM_weights.h5\n",
      " - 2s - loss: 0.0197 - val_loss: 4.7114e-04\n",
      "Epoch 79/100\n",
      "Epoch 00079: val_loss did not improve\n",
      " - 2s - loss: 0.0194 - val_loss: 0.0011\n",
      "Epoch 80/100\n",
      "Epoch 00080: val_loss did not improve\n",
      " - 2s - loss: 0.0197 - val_loss: 0.0010\n",
      "Epoch 81/100\n",
      "Epoch 00081: val_loss did not improve\n",
      " - 2s - loss: 0.0195 - val_loss: 6.6589e-04\n",
      "Epoch 82/100\n",
      "Epoch 00082: val_loss did not improve\n",
      " - 2s - loss: 0.0192 - val_loss: 0.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "Epoch 00083: val_loss did not improve\n",
      " - 2s - loss: 0.0192 - val_loss: 0.0020\n",
      "Epoch 84/100\n",
      "Epoch 00084: val_loss did not improve\n",
      " - 2s - loss: 0.0194 - val_loss: 9.4949e-04\n",
      "Epoch 85/100\n",
      "Epoch 00085: val_loss did not improve\n",
      " - 2s - loss: 0.0190 - val_loss: 0.0017\n",
      "Epoch 86/100\n",
      "Epoch 00086: val_loss did not improve\n",
      " - 2s - loss: 0.0188 - val_loss: 0.0028\n",
      "Epoch 87/100\n",
      "Epoch 00087: val_loss did not improve\n",
      " - 2s - loss: 0.0193 - val_loss: 9.3599e-04\n",
      "Epoch 88/100\n",
      "Epoch 00088: val_loss did not improve\n",
      " - 2s - loss: 0.0189 - val_loss: 8.2425e-04\n",
      "Epoch 89/100\n",
      "Epoch 00089: val_loss did not improve\n",
      " - 2s - loss: 0.0191 - val_loss: 0.0045\n",
      "Epoch 90/100\n",
      "Epoch 00090: val_loss did not improve\n",
      " - 2s - loss: 0.0188 - val_loss: 0.0011\n",
      "Epoch 91/100\n",
      "Epoch 00091: val_loss did not improve\n",
      " - 2s - loss: 0.0187 - val_loss: 8.6387e-04\n",
      "Epoch 92/100\n",
      "Epoch 00092: val_loss did not improve\n",
      " - 2s - loss: 0.0187 - val_loss: 5.9104e-04\n",
      "Epoch 93/100\n",
      "Epoch 00093: val_loss did not improve\n",
      " - 2s - loss: 0.0188 - val_loss: 0.0016\n",
      "Epoch 94/100\n",
      "Epoch 00094: val_loss improved from 0.00047 to 0.00045, saving model to ../projects/forecast/models/LSTM_weights.h5\n",
      " - 2s - loss: 0.0188 - val_loss: 4.5476e-04\n",
      "Epoch 95/100\n",
      "Epoch 00095: val_loss did not improve\n",
      " - 2s - loss: 0.0187 - val_loss: 0.0011\n",
      "Epoch 96/100\n",
      "Epoch 00096: val_loss did not improve\n",
      " - 2s - loss: 0.0187 - val_loss: 0.0026\n",
      "Epoch 97/100\n",
      "Epoch 00097: val_loss did not improve\n",
      " - 2s - loss: 0.0185 - val_loss: 0.0019\n",
      "Epoch 98/100\n",
      "Epoch 00098: val_loss did not improve\n",
      " - 2s - loss: 0.0186 - val_loss: 5.7160e-04\n",
      "Epoch 99/100\n",
      "Epoch 00099: val_loss did not improve\n",
      " - 2s - loss: 0.0185 - val_loss: 0.0013\n",
      "Epoch 100/100\n",
      "Epoch 00100: val_loss did not improve\n",
      " - 2s - loss: 0.0187 - val_loss: 8.7431e-04\n",
      "Result on validation data:  None\n"
     ]
    }
   ],
   "source": [
    "y= p.data_y[::-1]\n",
    "m.fit(y[:900],y[900:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3031ff0d5d7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "m.forecast(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x= m.param['memory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4097,    0, 3846, 3762, 3346, 3533, 3317, 4019,    0, 5197, 5735,\n",
       "       5223, 5558, 4665, 4797,    0, 4359, 3650, 3797, 3897, 3808, 3530,\n",
       "          0, 5054, 5042, 4767, 4427, 4852, 4406,    0, 4395, 3558, 3464,\n",
       "       3769, 3706, 4364,    0, 6102, 5011, 4782, 5020, 5263])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4097,    0, 3846, 3762, 3346, 3533, 3317, 4019,    0, 5197, 5735,\n",
       "       5223, 5558, 4665, 4797,    0, 4359, 3650, 3797, 3897, 3808, 3530,\n",
       "          0, 5054, 5042, 4767, 4427, 4852, 4406,    0, 4395, 3558, 3464,\n",
       "       3769, 3706, 4364,    0, 6102, 5011, 4782, 5020, 5263])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5263, 5020, 4782, 5011, 6102])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.data_y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
